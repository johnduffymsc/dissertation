\section{Introduction}

In his 1937 seminal paper "On Computable Numbers, with an Application to the Entscheidungsproblem" Alan Turing imagined a \emph{univeral computing machine} capable of performing any conceivable mathematical operation. Turing proved that by formulating a mathematical problem as an algorithm, consisting of a sequence of numbers and operations on these numbers, on an infinitely long tape, and with operations to move the tape left and right, it was possible to mechanise the computation of any problem. These machines became known as Turing Machines. 

Today's computers are Turing Machines. Turing's original sequence of numbers and operations are now referred to as the data and  instructions contained within a computer program. The infinitely long tape is now referred to as a computer's memory. And the set of instructions which manipulate program data, and which also permit access to the full range of available memory (move the tape left and right), are referred to as a computer's \emph{instruction set}.

High Performance Computing (HPC) is the solving of numerical problems which are beyond the capabilities of desktop and laptop computers in terms of the amount of data to be processed and the speed of computation required. For example, numerical weather forecasting (NWF) uses a grid of 3D positions to model a section of the Earth's atmosphere, and then solves partial differential equations at each of these points to produce forecasts. The processing performance and memory required to model such systems far exceeds that of even a high-end desktop. 

The UK Met Office use a number of grids to model global and and UK weather. The finest UK grid being a 1.5 km spaced 622 x 810 point inner grid, with a 4 km spaced 950 x 1025 outer grid, both with 70 vertical levels. To model the atmosphere on these grids the UK Met Office currently uses three Cray XC40 supercomputers, capable of 14 Petaflops ($10^{15}$ Floating Point Operations per Second), and which contain 460,000 computer cores, 2 Petabytes of memory and 24 Petabytes of storage.

Clearly a single Cray XC40 used for NWF is a somewhat different beast than a single imaginary Turing Machine. Some of the differences obviously relate to the imaginary nature of the Turing Machine, with its infinitely long tape, and some to what it is possible to build within the limits of today's technology. The Cray XC40's 2 Petabytes of memory is large, but not infinite. But possibly the most important differences are architectural. Each Cray XC40 is a massively parallel supercomputer, made up of a large number of individual processing nodes. Each node has a large but finite amount of processing capacity and memory. The problem data and program instructions must be divided up and distributed amongst the nodes. The nodes must be able to communicate in an efficient manner. And opportunities for \emph{parallel} and \emph{concurrent} processing should be exploited to minimise processing time. Each of these differences is a requirement to map HPC workloads onto a real-world machine. And each of these difference introduces a degree of complexity.   

Since the birth of electronic computing, there has always been a need to know long it will take for a computer to perform a particular task. This may be solely related to allocating computer time efficiently, or simply just wanting to know how long a program will take to run. Or, it may be commercially related; even a moderately sized single computer can be a large investment requiring the maximum performance possible for the purchase price. And more recently, the need to know how much processing power per unit of electricity a computer can achieve has become an important metric. This need for information is addressed by using a benchmark.

A benchmark is a standardised measure of performance. In computing terms this is a piece of software which performs a known task, and which tests a particular aspect(s) of computer performance. One aspect may be raw processing performance. High Performance Linpack (HPL) is one such benchmark, which produces a single measure of Floating Point Operations per Second (Flops) for a single, or more commonly, a cluster of computers. To address the complexity of design of modern supercomputers, as discussed above, a number of complementary benchmarks have been introduced, namely HPC Challenge (HPCC) and HP Conjugate Gradient (HPCG). HPC Challenge is a suite of benchmarks which measure processing performance, memory bandwidth, and network latency and bandwidth, to give a broad view of likely real-world application performance. HP Conjugate Gradient is intended to measure the performance of modern HPC workloads.

To put benchmark results into context, and to extrapolate from the results where performance gains might be realised, it is necessary to have an understanding of the main components of a computer and the network connecting a cluster of computers. The following sections of this chapter describe these components and the network in more detail.


%
% SECTION
%
\section{Computer Architecture}

\subsection{CPU}
The CPU (Central Processing Unit) is the hardware that executes program instructions. Program data is loaded from main memory into CPU registers, the program instructions operate on the registers, and then the results are stored back in main memory. Registers may be general purpose registers, or have a specific use, such as floating point registers for fast floating point operations. A special purpose register called the Instruction Pointer points to the next instruction to be executed. A modern CPU will typically have multiple processing cores, each with its own set of registers.


\subsection{Processes}
A \emph{process} is a running program executing on a CPU, or on a core of multi-core CPU. At any time each process has a \emph{state}. This state includes the current contents of the registers and the Instruction Pointer. A multi-core CPU can run multiple processes simultaneously. A process may be a user program, such as a benchmark, or an operating system process.

The single-threaded benchmarks used in this project run as a single process, one process per CPU core.   


\subsection{Threads}
A \emph{thread}, sometimes referred to as a \emph{lightweight process}, is the minimum amount of work that can be \emph{scheduled} by a CPU. Scheduling is discussed shortly. A process may consist of multiple threads, each of which shares the address space of the process. Starting and stopping a thread is less expensive than starting and stopping a process. And because a process address space is shared between threads, data sharing between threads can be less expensive and easier than other mechanisms of inter-process communication. It is for these reasons that multi-threaded programs are used. However, multi-threaded programs can be difficult to write, and data sharing between threads must be considered carefully to avoid \emph{data races}, \emph{livelocks} and \emph{deadlocks}.

The multi-threaded benchmarks used in this project use OpenMP to parallelise computation. A single process is run on each node, with the benchmark work being distributed across cores by OpenMP using threads. 


\subsection{Context Switch}

A \emph{context switch} is the suspension of a running process and the starting, or resuming, of a different process. Context switching is implemented for a number of reasons; to share access to the CPU across multiple processes (\emph{concurrency}), whenever a program issues a \emph{system call} to request a service from the \emph{kernel}, whenever the \emph{kernel} receives an \emph{interrupt} from a timer or peripheral device and requires access to the CPU, and to avoid wasting processor time when waiting for slow Input/Output (IO) operations.

Whenever a context switch takes place, the current \emph{state} of the running process is saved to main memory, and the \emph{state} of the new process is retrieved from main memory. This takes time.

LMTIME...

Linux uses a mechanism called vDSO (Virtual Dynamic Shared Object) to avoid a context switch whenever a \emph{system call} is made which does not require an elevation of system privileges. However, on the Arm64 architecture this only includes the \emph{clock\_gettime()} and \emph{gettimeofday()} system calls, so effectively all system calls involve a context switch.

As discussed in Chapter 5, each packet of network data sent between cluster nodes generates a network card \emph{interrupt} on the receiving node. This \emph{interrupt} requires \emph{servicing} by the kernel, which involves a context switch from the benchmark process to a kernel process. And this takes a considerable amount of time and drastically affects benchmark performance. The Networking section of this chapter discusses measures to mitigate this performance penalty.

\subsection{Concurrency and Parallelism}

\emph{Concurrency} and \emph{Parallelism} are similar concepts and refer to a computer running multiple processes at the same time, or the illusion of this. A single core CPU can only run a single process at a time. However, if the context switching between processes is fast enough, then this may result in the illusion of \emph{concurrency}. This is sometimes referred to to as \emph{time-slicing} or \emph{time-sharing}. But this is not \emph{parallelism}. \emph{Parallelism} is the simultaneous running of multiple processes, which requires multiple cores or multiple computers.

A benchmark will typically be running the same process in parallel on each node, which each node operating on a different portion of the benchmark data.


\subsection{Interrupts}

Modern operating systems, such as Linux, are \emph{event driven}. This means that instead of continuously looping over a list of actions, the operating system performs actions when events occur. Events generate \emph{interrupts} which cause the operating system to pause the running process and \emph{service} the interrupt. Interrupts may be hardware interrupts, such as the interrupt generated by a network card upon receipt of a data packet, or may be generated by software, \emph{software interrupts}.

There are a number of sources of interrupts, but the main source of interrupts for the operating system is the system clock. The clock of the BCM2711 ticks at a frequency of 1.5 GHz, and at predetermined counts of the system clock the operating system performs predetermined actions. Other sources of interrupts include user input from the keyboard, hard disk IO and environmental sensors.

Interrupts generated by network activity, and the effects of this on benchmark performance, are discussed shortly.


\subsection{Kernel Preemption Model}

The \emph{kernel preemption model} is related to process and thread scheduling. Scheduling can either be \emph{preemptive} or \emph{non-preemptive}. A pre-emptive scheduler can interrupt a running thread or process, based upon a \emph{scheduling policy}, to enable a different thread or process to run. Scheduling policies include \emph{First-Come First-Served}, \emph{Round Robin}, and \emph{Priority-Driven Scheduling}. Non-preemptive scheduling does not interrupt running threads or processes. The kernel preemption model is a kernel configuration option set during kernel compilation.

Linux supports three kernel preemption models, \emph{preemptive}, \emph{voluntary preemption} and \emph{non-preemptive}. The \emph{preemptive} model is used where \emph{low latency} is the primary requirement, such as for audio recording. This model prioritises latency over processing throughput. The \emph{non-preemptive} model priorities processing throughput over latency, and it used where maximum processing power is required. The \emph{voluntary preemption} model is a compromise between the other two models, and is typically used for desktop systems where the user requires responsive mouse and keyboard input and also no excessive reduction in performance.

As previously stated, the kernel preemption model is a kernel configuration option. Quoting the \emph{help} associated with the \verb|CONFIG_PREEMPT_NONE| kernel configuration option:

\say{This is the traditional Linux preemption model, geared towards throughput. It will still provide good latencies most of the time, but there are no guarantees and occasional longer delays are possible. Select this option if you are building a kernel for a server or scientific/computation system, or if you want to maximise the raw processing power of the kernel, irrespective of scheduling latencies.}

The preemption model of default kernel installed with Ubuntu 20.04 LTS 64-bit is \emph{voluntary preemption}. The recompilation of the Linux kernel as a \emph{non-preemptive} kernel to maximise raw benchmark processing power is discussed in Chapter 5.  


\subsection{Main Memory}

Main memory is the largest component of the memory system of a computer. On desktop, laptop and larger computers, the memory chips usually reside on small circuit boards that fit into sockets on the computer mainboard. These can be upgraded in size by the user. On some smaller computers, such as the Raspberry Pi 4, the memory chip is soldered onto the computer circuit board and is not upgradable.

Each memory location contains a byte of data, where a byte is 8 binary bits. Bytes are stored sequentially at an \emph{addresses}, which is a binary number in the range 0 up to the maximum address supported by the system. The maximum address typically aligns with the register size. For example, 64-bit computer has 64-bit registers which can hold an address in the range 0 to $64^2$. This requires a 64-bit physical \emph{address bus} to address each byte of memory. Practical considerations sometimes limit the size of the address bus. The Raspberry Pi 4 is a 64-bit computer but has a 48-bit physical address bus.

Computers systems without an operating system, such as embedded systems, permit direct access to main memory from software. In this case there is a direct mapping between the memory address within a computer program and the physical address in main memory. Most operating systems present an abstracted view of main memory to each program running on the system. This is called \emph{virtual memory}.
 
    
\begin{figure}
	\centering	
	\includegraphics[width=0.9\textwidth]{virtual-memory.pdf}
	\caption{\textbf{Virtual Memory}. Each process has a \emph{virtual address} space mapped to main memory in \emph{pages} by a \emph{page table} which resides in main memory. A smaller page table called the \emph{Translation Lookaside Buffer} (TLB) is a \emph{cache} in close proximity to each core. The TLB enables fast lookup of physical page addresses without resorting to a slower lookup in the main memory page table.}
\end{figure}


\subsection{Virtual Memory}

Virtual memory is the abstracted view of main memory presented to a running program by the operating system. Virtual memory requires both hardware support, through the Memory Management Unit (MMU), and software support by the operating system. Contiguous regions of virtual memory are organised into \emph{pages}, typically 4 KB in size. Each page of virtual memory maps to a page of physical memory through a \emph{page table} which resides in main memory. A smaller page table called the \emph{Translation Lookaside Buffer} (TLB), which is a \emph{cache} in close proximity to each processing core, is discussed later.

There are a number of benefits of implementing virtual memory. One is to permit the use of a smaller amount of physical memory than is actually addressable. In this case, pages currently in use reside in main memory, and pages no longer required are \emph{swapped} to permanent storage to make space for new pages. This illusion of a full amount of addressable main memory is transparent to the user. But the \emph{paging} between main memory and permanent storage is slow, and is therefore not used in HPC applications.

Possibly the most important benefit of using virtual memory is to implement a protection mechanism called \emph{process isolation}. Each running program, or \emph{process}, executes in its own private, virtual address space. This means that it is not possible for a process to overwrite memory in the address space of another process, possibly due a bug in a program. This process isolation in managed by the operating system using virtual memory. It is possible for multiple processes to communicate through \emph{shared memory}, where each process can read and write to the same block of memory, but this requires programs to be specifically written to make use of this mechanism. 

  
\subsection{Caches}

If we imagine Turing's infinitely long tape and the inertia that must be overcome to move such a tape left and right, it would not be too much of a leap of the imagination to propose copying some sequential part of the tape onto a finite, lighter tape which could be moved left and right faster. Then if the data required for the current part of our computation was contained within this faster tape, the computation would be conducted faster. The contents of the finite tape would be refreshed with data from the infinite tape as required, which may be expensive in terms of time. And if the speed at which we can perform operations on the lighter tape began to outpace the speed of movement of the tape, we might propose copying some of the data onto an even shorter, even faster tape.

If we replace speed of tape movement with speed of memory access, then this imaginary situation is analogous to the layering of memory within a real computer system. Main memory access is slow compared to processor computing speed, so main memory is copied into smaller, faster \emph{caches} colocated on the same silicon die as the processing cores. Each level of cache closer to a processing core is smaller but faster than the previous, with the cache closest to the processing core being referred to as Level 1 (L1) cache. A processor may have L1, L2 and L3 caches, the outer cache possibly being shared between a number of processing cores. As we shall discuss later in this chapter, the speed at which program data flows from main memory through the caches to the processing cores is critical for application performance, and considerable care is taken to minimise \emph{cache misses} which require a \emph{cache refresh} from main memory.

There are typically three types of cache, the \emph{instruction cache}, the \emph{data cache} and the \emph{Translation Lookaside Buffer}, each of which may be in L1, L2 or L3 proximity to a processing core, or in the case of the TLB proximity to the Memory Management Unit (MMU) . A \emph{unified} cache is a combined instruction and data cache. The \emph{instruction cache} holds program instructions laid out in memory in close proximity to the instruction currently being executed. Similarly, the \emph{data cache} holds program data laid out in memory in close proximity to the data currently in use. In both cases, if the next instruction or next piece of data required is found in a cache, a \emph{cache hit}, this information can be accessed very quickly. If the information in not in a cache, a \emph{cache miss}, then an expensive \emph{cache refresh} from main memory is required. The \emph{Translation Lookaside Buffer} (TLB) is a small page table which enables fast lookup of required pages in main memory. If the address of required page is not in a TLB then an expensive main memory page table lookup is required.   

Data (program instructions, program data, or page addresses) in a cache are a copy of data in main memory. In addition to the data itself, the location of the data in main memory must also be stored in the cache. This additional information increases the physical size of the cache and is expensive in terms transistor count and silicon surface area. To address this problem, rather than storing the location of each element of data, the data in the cache is organised into \emph{cache lines}. This reduces the number of locations to store, and also has the additional benefit that a request for a new piece of data will also bring into the cache data in close proximity which fit into a cache line.

A cache may be \emph{fully-associative} or \emph{set-associative}. In a \emph{fully-associative} cache every memory address can be stored in all cache entries, i.e. a cache entry can point to any memory address. A \emph{set-associative} cache restricts memory addresses to a set of entries in the cache, i.e. certain cache entries can only point to certain memory addresses. The use of a \emph{fully-associative cache} versus a \emph{set-associative} cache is a compromise between a high probability of a cache hit but slower lookup, the \emph{fully-associative cache}, and a lower probability of a \emph{cache hit} but faster lookup, the \emph{set-associative cache}.      

The BCM2711 contains the following caches, each with 64 byte cache lines:
\begin{itemize}
\item 48 KB 3-way set-associative L1 instruction cache per core
\item 32 KB 2-way set-associative L1 data cache per core
\item 1 MB 16-way set-associative shared L2 unified cache
\end{itemize}

The BCM2711 MMU contains the following caches:
\begin{itemize}
\item 48-entry fully-associative L1 instruction TLB
\item 32-entry fully-associative L1 data TLB
\item 4-way set-associative 1024-entry L2 TLB in each processor
\end{itemize}

The appropriate layout of data in memory, in either \emph{row-major} or \emph{column-major} ordering, and subsequent program access pattern, via the \emph{data cache}, has a major impact on program performance. If the data layout matches the access pattern, then each \emph{cache refresh} will fill a \emph{cache line} with the data required and also the data likely to be required next in sequential order. A \emph{cache refresh} will only be required once the entire \emph{cache line} has been used. If the data layout does not match the access pattern, then data will be moved in and out of the \emph{data cache} without being used, and this will result in an unnecessarily high number of expensive \emph{cache misses}. 


 
%
% Arm Architecture
%
\section{ARM Architecture}

Ever since the introduction of the ARMv8-A architecture in 2011, Arm processors have become an increasingly popular choice for High Performance Computing (HPC) due to improvements in the ARMv8 instruction set specifically targeting HPC combined with low power requirements. The 415 Petaflops Fugaku supercomputer, based on the Fujitsu AX64 Arm-based processor, topped the June 2020 Top500 List and also the November 2019 Green500 List.

Arm processors are based upon RISC (Reduced Instruction Set Computer) principles with a simple Load/Store architecture. Data is loaded from main memory into processor registers, computations are performed using fixed-length instructions, and the results are then stored back in main memory. This compares with CISC (Complicated Instruction Set Computer) architectures with complicated memory addressing and computation modes with variable length instructions. The simplicity of design, which directly translates to a low transistor count, results in high performance combined with low power requirements.

The ARMv8-A in introduced the first Arm 64-bit architecture.

armv8.1...

armv8.2...

SVE...

Fugaku chip...

Fujitsu chip... 








%
% SECTION
%
\section{Networking}

\subsection{Ethernet}

\subsection{MTU}

\subsection{Interrupt Coalescing}

\subsection{Receive Side Scaling}

\subsection{Receive Packet Steering}

\subsection{Receive Flow Steering}


%
% SECTION
%
\section{Benchmarks}

\subsection{Landscape}

High Performance Linpack (HPL) is the industry standard HPC benchmark and has been for since 1993. It is used by the Top500 and Green500 lists to rank supercomputers in terms of raw performance and performance per Watt, respectively. However, it has been criticised for producing a single number, and not being a true measure of real-world application performance. This has led to the creation of complementary benchmarks, namely HPC Challenge (HPCC) and High Performance Conjugate Gradients (HPCG). These benchmarks measure whole system performance, including processing power, memory bandwidth, and network speed and latency, in relation to standard HPC algorithms such as FFT and CG.


%
% SUB SECTION
%
\subsection{High Performance Linpack (HPL)}

HPL did not begin life as a supercomputer benchmark. LINPACK is a software package for solving Linear Algebra problems. And in 1979 the ``LINPACK Report'' appeared as an appendix to the LINPACK User Manual. It listed the performance of 23 commonly used computers of the time when solving a matrix problem of size 100. The intention was that users could use this data to extrapolate the execution time of their matrix problems.

As technology progressed, LINPACK evolved through LINPACK 100, LINPACK 1000 to HPLinpack, developed for use on parallel computers. High Performance Linpack (HPL) is an implementation of HPLinpack.

In 1993 the Top500 List was created to rank the performance of supercomputers and HPL was used, and still is used, to measure performance and create the rankings.

HPL solves a dense system of equations of the form:

\[A\mathbf{x}=\mathbf{b}\]

HPL generates random data for a problem size N. It then solves the problem using LU decomposition and partial row pivoting.

HPL requires an implementation of MPI (Message Passing Interface) and a BLAS (Basic Linear Algebra Subroutines) library to be installed. For this project, OpenMPI was the MPI implementation used, and OpenBLAS and BLIS were the BLAS libraries used. Both BLAS libraries were used in the single-threaded serial version and also the multi-threaded OpenMP version.

In HPL terminology, $R_{peak}$ is the theoretical maximum performance. And $R_{max}$ is the maximum achieved performance, which will normally be observed using the maximum problem size $N_{max}$.


%
% SUB SECTION
%
\subsubsection{Determining Input Parameters}

The main parameters which affect benchmark results are the block size NB, the problem size N, and the processor grid dimensions P and Q.

The block size NB is used for two purposes. Firstly, to ``block'' the problem size matrix of dimension N x N into sub-matrices of dimension NB x NB. This is described in more detail in the Section ??. And secondly, as the message size (or multiples of) for distributing data between cluster nodes.

The optimum size for NB is related to the BLAS library \verb|dgemm| \emph{kernel} block size, which is related to CPU register and L1, L2, and L3 (when available) cache sizes. But this is not easily determined as a simple multiple of the \emph{kernel} block size. Some experimentation is required to determine the optimum size for NB.

\href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions} suggests NB should be in the range 32 to 256. A smaller size is better for data distribution latency, but may result in data not being available in sufficiently large chunks to be processed efficiently. Too high a value may result in data starvation while nodes wait for data due to network latency.

For this project, HPL benchmarks were run with NB in the range 32 to 256 in order to determine the optimum size for the Aerin Cluster, and for each BLAS library in serial and OpenMP versions. 

For maximum data processing efficiency, and therefore optimum benchmark performance, the problem size N should be as large as possible. This optimises the cluster processing/communications ratio. Optimum efficiency is achieved when the problem size utilises 100\% of memory. But this is never actually achievable, since the operating system and benchmark software require memory to run. \href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions} suggests 80\% of total available memory as a good starting point, and this value was used for this project.

For optimum benchmark performance the problem size N needs to be an integer multiple of the block size NB. This ensures every NB x NB sub-matrix is a full sub-matrix of the N x N problem size, i.e. there are no partially full NB x NB sub-matrices at N x N matrix boundaries.

For each value of NB, the following formula is used to determine the problem size N, taking into account 80\% memory usage:

\[N = \left[\left(0.8 \sqrt{\frac{\text{Memory in GB} \times 1024^3}{8}}\right) \div NB\right] \times NB\]

The division by 8 in the inner parenthesis is the size in bytes of a double precision floating point number.


The online tool \href{http://hpl-calculator.sourceforge.net}{HPL Calculator} by Mohammad Sindi automates the process of calculating the problem size N for block sizes NB in the range 96 to 256, and for memory usage 80\% to 100\%.

The values of N determined using HPL Calculator were cross-checked with the formula above.

The processor grid dimensions P and Q represent a P x Q grid of processor cores. For example, the Aerin cluster has 8 nodes, each with 4 cores, giving a total of 32 processor cores. These core can be organised in compute grids of 1 x 32, 2 x 16 and 4 x 8.

The HPL algorithm favours P x Q grids as square as possible, i.e. with P almost equal to Q, but with P smaller than Q. So, for a single node with 4 cores, a processor grid of 1 x 4 gives better benchmark performance than 2 x 2.

If the Aerin Cluster used a high speed interconnect between nodes, such as InfiniBand, as used on large HPC clusters, maximum performance would be expected to be achieved using a processor grid of 4 x 8. This is the ``squarest'' possible P x Q grid using 32 cores whilst maintaining P less than Q. However, as noted in \href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions}, Ethernet is not a high speed interconnect. An Ethernet network is simplistically a single wire connecting the nodes, with each node competing (using random transmission times) for access to the wire to transmit data. This physical limitation reduces potential maximum cluster performance, and the maximum achievable performance is seen using a flatter P x Q grid. This proved to be the case, and maximum cluster performance was observed using a processor grid of 2 x 16 for all 8 nodes. This phenomena was also observed using when using less than 8 nodes.


%
% SECTION
%
\subsection{HPC Challenge (HPCC)}

HPCC is a suite of benchmarks which test different aspects of cluster performance. These benchmarks include tests for processing performance, memory bandwidth, and network bandwidth and latency. HPCC is intended to give a broader view of cluster performance than HPL alone, which should reflect real-world application performance more closely. HPCC includes HPL as one of the suite of benchmarks.

The HPCC suite consists of the following 7 benchmarks, where \emph{single} indicates the benchmark is run a single randomly selected node, \emph{star} indicates the benchmark in run independently on all nodes, and \emph{global} indicates the benchmark is run using all nodes in a coordinated manner. 


%
% SUB SECTION
%
\subsubsection{HPL}

HPL is a \emph{global} benchmark which solves a dense system of linear equations.


%
% SUB SECTION
%
\subsubsection{DGEMM}

The DGEMM benchmark tests double precision matrix-matrix multiplication performance in both \emph{single} and \emph{star} modes.

 
%
% SUB SECTION
%
\subsubsection{STREAM}

The STREAM benchmark tests memory bandwidth, to and from memory, in both \emph{single} and \emph{star} modes.


%
% SUB SECTION
%
\subsubsection{PTRANS}

PTRANS, Parallel Matrix Transpose, is a \emph{global} benchmark which tests system performance in transposing a large matrix.


%
% SUB SECTION
%
\subsubsection{RandomAccess}

The RandomAccess benchmark tests the performance of random updates to a large table in memory, in \emph{single}, \emph{star}, and \emph{global} modes.


%
% SUB SECTION
%
\subsubsection{FFT}

FFT tests the Fast Fourier Transform performance of a large vector, in \emph{single}, \emph{star}, and \emph{global} modes.


%
% SUB SECTION
%
\subsubsection{Network Bandwidth and Latency}

This benchmark measures network/communications bandwidth and latency in \emph{global} mode.


%
% SECTION
%
\subsection{High Performance Conjugate Gradients (HPCG)}

HPCG is intended to be complementary to HPL, and to incentivise hardware manufacturers to improve computer architectures for modern HPC workloads. 

Quoting the Super Computing 2019 HPCG Handout:

\say{The HPC Conjugate Gradient (HPCG) benchmark uses a preconditioned conjugate gradient (PCG) algorithm to measure the performance of HPC platforms with respect to frequently observed, yet challenging, patterns of execution, memory access, and global communication.}

\say{The PCG implementation uses a regular 27-point stencil discretixation in 3 dimensions of an elliptic partial differential equation (PDE) with zero Dirichlet boundary condition. The 3-D domain is scaled to fill a 3-D virtual process grid of all available MPI process ranks. The CG iteration includes a local and symmetric Gauss-Seidel preconditioner, which computes a forward and a back solve with a triangular matrix. All of these features combined allow HPCG to deliver a more accurate performance metric for modern HPC architectures.}






\section{Introduction}

In his 1937 seminal paper "On Computable Numbers, with an Application to the Entscheidungsproblem" Alan Turing imagined a \emph{univeral computing machine} capable of performing any conceivable mathematical operation. Turing proved that by formulating a mathematical problem as an algorithm, consisting of a sequence of numbers and operations on these numbers, on an infinitely long tape, and with operations to move the tape left and right, it was possible to mechanise the computation of any problem. These machines became known as Turing Machines. 

Today's computers are Turing Machines. Turing's original sequence of numbers and operations are now referred to as the data and  instructions contained within a computer program. The infinitely long tape is now referred to as a computer's memory. And the set of instructions which manipulate program data, and which also permit access to the full range of available memory (move the tape left and right), are referred to as a computer's \emph{instruction set}.

High Performance Computing (HPC) is the solving of numerical problems which are beyond the capabilities of desktop and laptop computers in terms of the amount of data to be processed and the speed of computation required. For example, numerical weather forecasting (NWF) uses a grid of 3D positions to model a section of the Earth's atmosphere, and then solves partial differential equations at each of these points to produce forecasts. The processing performance and memory required to model such systems far exceeds that of even a high-end desktop. 

The UK Met Office use a number of grids to model global and and UK weather. The finest UK grid being a 1.5 km spaced 622 x 810 point inner grid, with a 4 km spaced 950 x 1025 outer grid, both with 70 vertical levels. To model the atmosphere on these grids the UK Met Office currently uses three Cray XC40 supercomputers, capable of 14 Petaflops ($10^{15}$ Floating Point Operations per Second), and which contain 460,000 computer cores, 2 Petabytes of memory and 24 Petabytes of storage.

Clearly a single Cray XC40 used for NWF is a somewhat different beast than a single imaginary Turing Machine. Some of the differences obviously relate to the imaginary nature of the Turing Machine, with its infinitely long tape, and some to what it is possible to build within the limits of today's technology. The Cray XC40's 2 Petabytes of memory is large, but not infinite. But possibly the most important differences are architectural. Each Cray XC40 is a massively parallel supercomputer, made up of a large number of individual processing nodes. Each node has a large but finite amount of processing capacity and memory. The problem data and program instructions must be divided up and distributed amongst the nodes. The nodes must be able to communicate in an efficient manner. And opportunities for \emph{parallel} and \emph{concurrent} processing should be exploited to minimise processing time. Each of these differences is a requirement to map HPC workloads onto a real-world machine. And each of these difference introduces a degree of complexity.   

Since the birth of electronic computing, there has always been a need to know long it will take for a computer to perform a particular task. This may be solely related to allocating computer time efficiently, or simply just wanting to know how long a program will take to run. Or, it may be commercially related; even a moderately sized single computer can be a large investment requiring the maximum performance possible for the purchase price. And more recently, the need to know how much processing power per unit of electricity a computer can achieve has become an important metric. This need for information is addressed by using a benchmark.

A benchmark is a standardised measure of performance. In computing terms this is a piece of software which performs a known task, and which tests a particular aspect(s) of computer performance. One aspect may be raw processing performance. High Performance Linpack (HPL) is one such benchmark, which produces a single measure of Floating Point Operations per Second (Flops) for a single, or more commonly, a cluster of computers. To address the complexity of design of modern supercomputers, as discussed above, a number of complementary benchmarks have been introduced, namely HPC Challenge (HPCC) and HP Conjugate Gradient (HPCG). HPC Challenge is a suite of benchmarks which measure processing performance, memory bandwidth, and network latency and bandwidth, to give a broad view of likely real-world application performance. HP Conjugate Gradient is intended to measure the performance of modern HPC workloads.

To put benchmark results into context, and to extrapolate from the results where performance gains might be realised, it is necessary to have an understanding of the main components of a computer and the network connecting a cluster of computers. The following sections of this chapter describe these components and the network in more detail.

Matrix-matrix multiplication plays an important role in computer benchmarking because the multiplication of large matrices tests processing, memory, and network performance. A more detailed discussion of this topic is also included in this chapter. 


%
% SECTION
%
\section{CPU Architecture}

\subsection{Threads}

\subsection{Processes}

\subsection{Context Switch}

\subsection{Concurrency}

\subsection{Parallel Computation}

\subsection{Scheduling}

\subsection{Interrrupts}

\subsection{Kernel Preemption Model}

\subsection{ARM Architecture }

Fugatku...

Numer of Arm-based in Top/Green 500...

48 core workstation...

RISC paper...

RISC/CISC

Load/Store architecture

Simplicity of design...

Transistor count...

Electrical power...

armv7...

armv8... 64-bit

armv8.1...

armv8.2...

SVE...

Fugaku chip...

Fujitsu chip... 


%
% SECTION
%
\section{Main Memory}

Main memory is the largest component of the memory system of a computer. On desktop, laptop and larger computers, the memory chips usually reside on small circuit boards that fit into sockets on the computer mainboard. These can be upgraded in size by the user. On some smaller computers, such as the Raspberry Pi 4, the memory chip is soldered onto the computer circuit board and is not upgradable.

Each memory location contains a byte of data, where a byte is 8 binary bits. Bytes are stored sequentially at an \emph{addresses}, which is a binary number in the range 0 up to the maximum address supported by the system. The maximum address typically aligns with the register size. For example, 64-bit computer has 64-bit registers which can hold an address in the range 0 to $64^2$. This requires a 64-bit physical \emph{address bus} to address each byte of memory. Practical considerations sometimes limit the size of the address bus. The Raspberry Pi 4 is a 64-bit computer but has a 48-bit physical address bus.

Computers systems without an operating system, such as embedded systems, permit direct access to main memory from software. In this case there is a direct mapping between the memory address within a computer program and the physical address in main memory. Most operating systems present an abstracted view of main memory to each program running on the system. This is called \emph{virtual memory}.
 
    
\begin{figure}
	\centering	
	\includegraphics[width=0.9\textwidth]{virtual-memory.pdf}
	\caption{\textbf{Virtual Memory}. Each process has a \emph{virtual address} space mapped to main memory in \emph{pages} by a \emph{page table} which resides in main memory. A smaller page table called the \emph{Translation Lookaside Buffer} (TLB) is a \emph{cache} in close proximity to each core. The TLB enables fast lookup of physical page addresses without resorting to a slower lookup in the main memory page table.}
\end{figure}

\subsection{Virtual Memory}

Virtual memory is the abstracted view of main memory presented to a running program by the operating system. Virtual memory requires both hardware support, through the Memory Management Unit (MMU), and software support by the operating system. Contiguous regions of virtual memory are organised into \emph{pages}, typically 4 KB in size. Each page of virtual memory maps to a page of physical memory through a \emph{page table} which resides in main memory. A smaller page table called the \emph{Translation Lookaside Buffer} (TLB), which is a \emph{cache} in close proximity to each processing core, is discussed later.

There are a number of benefits of implementing virtual memory. One is to permit the use of a smaller amount of physical memory than is actually addressable. In this case, pages currently in use reside in main memory, and pages no longer required are \emph{swapped} to permanent storage to make space for new pages. This illusion of a full amount of addressable main memory is transparent to the user. But the \emph{paging} between main memory and permanent storage is slow, and is therefore not used in HPC applications.

Possibly the most important benefit of using virtual memory is to implement a protection mechanism called \emph{process isolation}. Each running program, or \emph{process}, executes in its own private, virtual address space. This means that it is not possible for a process to overwrite memory in the address space of another process, possibly due a bug in a program. This process isolation in managed by the operating system using virtual memory. It is possible for multiple processes to communicate through \emph{shared memory}, where each process can read and write to the same block of memory, but this requires programs to be specifically written to make use of this mechanism. 

  
\subsection{UMA versus NUMA}

\subsection{Shared Memory}

\subsection{Distributed Memory}


%
% SECTION
%
\section{Caches}

If we imagine Turing's infinitely long tape and the inertia that must be overcome to move such a tape left and right, it would not be too much of a leap of the imagination to propose copying some sequential part of the tape onto a finite, lighter tape which could be moved left and right faster. Then if the data required for the current part of our computation was contained within this faster tape, the computation would be conducted faster. The contents of the finite tape would be refreshed with data from the infinite tape as required, which may be expensive in terms of time. And if the speed at which we can perform operations on the lighter tape began to outpace the speed of movement of the tape, we might propose copying some of the data onto an even shorter, even faster tape.

If we replace speed of tape movement with speed of memory access, then this imaginary situation is analogous to the layering of memory within a real computer system. Main memory access is slow compared to processor computing speed, so main memory is copied into smaller, faster \emph{caches} colocated on the same silicon die as the processing cores. Each level of cache closer to a processing core is smaller but faster than the previous, with the cache closest to the processing core being referred to as Level 1 (L1) cache. A processor may have L1, L2 and L3 caches, the outer cache possibly being shared between a number of processing cores. As we shall discuss later in this chapter, the speed at which program data flows from main memory through the caches to the processing cores is critical for application performance, and considerable care is taken to minimise \emph{cache misses} which require a \emph{cache refresh} from main memory.

Caches... lines

Cache coherency...

TLB...

The MMU has the following features:

48-entry fully-associative L1 instruction TLB.
32-entry fully-associative L1 data TLB for data load and store pipelines.
4-way set-associative 1024-entry L2 TLB in each processor.

The L1 instruction memory system has the following features:

48KB 3-way set-associative instruction cache.
Fixed line length of 64 bytes.

The L1 data memory system has the following features:

32KB 2-way set-associative data cache.
Fixed line length of 64 bytes.

The features of the L2 memory system include:

Configurable L2 cache size of 512KB, 1MB, 2MB and 4MB.
Fixed line length of 64 bytes.
Physically indexed and tagged cache.
16-way set-associative cache structure.

The SCU uses hybrid Modified Exclusive Shared Invalid (MESI) and Modified Owned Exclusive Shared Invalid (MOESI) protocols to maintain coherency between the individual L1 data caches and the L2 cache.

The L2 memory system requires support for inclusion between the L1 data caches and the L2 cache. A line that resides in any of the L1 data caches must also reside in the L2 cache. However, the data can differ between the two caches when the L1 cache line is in a dirty state. If another agent, a core in the cluster or another cluster, accesses this line in the L2 then it knows the line is present in the L1 of a processor and then it queries that core for the most recent data.


%
% SECTION
%
\section{Networking}

\subsection{Ethernet}

\subsection{MTU}

\subsection{Interrupt Coalescing}

\subsection{Receive Side Scaling}

\subsection{Receive Packet Steering}

\subsection{Receive Flow Steering}


%
% SECTION
%
\section{Matrix Multipication}


%
% SECTION
%
\section{Benchmarks}

\subsection{High Performance Linpack}

\subsection{HPC Challenge}

\subsection{HP Conjugate Gradients}


%
% OLD STUFF FROM HERE TO WEED...
% 


%
% SECTION
%
\section{Landscape}

High Performance Linpack (HPL) is the industry standard HPC benchmark and has been for since 1993. It is used by the Top500 and Green500 lists to rank supercomputers in terms of raw performance and performance per Watt, respectively. However, it has been criticised for producing a single number, and not being a true measure of real-world application performance. This has led to the creation of complementary benchmarks, namely HPC Challenge (HPCC) and High Performance Conjugate Gradients (HPCG). These benchmarks measure whole system performance, including processing power, memory bandwidth, and network speed and latency, in relation to standard HPC algorithms such as FFT and CG.

HPL has been the main focus of this project, mainly because it is the industry standard HPC benchmark, but also because tuning performance for HPL will also produce optimum results for HPCC (HPCC includes HPL) and HPCG.

Because BLAS (Basic Linear Algebra Subroutine) library performance and cluster topology, pure OpenMPI and hybrid OpenMPI/OpenMP, have a direct impact on benchmark performance, a discussion of these topics is also included in this chapter. 


A detailed description of each benchmark follows.

%
% SECTION
%
\section{High Performance Linpack (HPL)}

HPL did not begin life as a supercomputer benchmark. LINPACK is a software package for solving Linear Algebra problems. And in 1979 the ``LINPACK Report'' appeared as an appendix to the LINPACK User Manual. It listed the performance of 23 commonly used computers of the time when solving a matrix problem of size 100. The intention was that users could use this data to extrapolate the execution time of their matrix problems.

As technology progressed, LINPACK evolved through LINPACK 100, LINPACK 1000 to HPLinpack, developed for use on parallel computers. High Performance Linpack (HPL) is an implementation of HPLinpack.

In 1993 the Top500 List was created to rank the performance of supercomputers and HPL was used, and still is used, to measure performance and create the rankings.

HPL solves a dense system of equations of the form:

\[A\mathbf{x}=\mathbf{b}\]

HPL generates random data for a problem size N. It then solves the problem using LU decomposition and partial row pivoting.

HPL requires an implementation of MPI (Message Passing Interface) and a BLAS (Basic Linear Algebra Subroutines) library to be installed. For this project, OpenMPI was the MPI implementation used, and OpenBLAS and BLIS were the BLAS libraries used. Both BLAS libraries were used in the single-threaded serial version and also the multi-threaded OpenMP version.

In HPL terminology, $R_{peak}$ is the theoretical maximum performance. And $R_{max}$ is the maximum achieved performance, which will normally be observed using the maximum problem size $N_{max}$.


%
% SUB SECTION
%
\subsection{Determining Input Parameters}

The main parameters which affect benchmark results are the block size NB, the problem size N, and the processor grid dimensions P and Q.

The block size NB is used for two purposes. Firstly, to ``block'' the problem size matrix of dimension N x N into sub-matrices of dimension NB x NB. This is described in more detail in the Section ??. And secondly, as the message size (or multiples of) for distributing data between cluster nodes.

The optimum size for NB is related to the BLAS library \verb|dgemm| \emph{kernel} block size, which is related to CPU register and L1, L2, and L3 (when available) cache sizes. But this is not easily determined as a simple multiple of the \emph{kernel} block size. Some experimentation is required to determine the optimum size for NB.

\href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions} suggests NB should be in the range 32 to 256. A smaller size is better for data distribution latency, but may result in data not being available in sufficiently large chunks to be processed efficiently. Too high a value may result in data starvation while nodes wait for data due to network latency.

For this project, HPL benchmarks were run with NB in the range 32 to 256 in order to determine the optimum size for the Aerin Cluster, and for each BLAS library in serial and OpenMP versions. 

For maximum data processing efficiency, and therefore optimum benchmark performance, the problem size N should be as large as possible. This optimises the cluster processing/communications ratio. Optimum efficiency is achieved when the problem size utilises 100\% of memory. But this is never actually achievable, since the operating system and benchmark software require memory to run. \href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions} suggests 80\% of total available memory as a good starting point, and this value was used for this project.

For optimum benchmark performance the problem size N needs to be an integer multiple of the block size NB. This ensures every NB x NB sub-matrix is a full sub-matrix of the N x N problem size, i.e. there are no partially full NB x NB sub-matrices at N x N matrix boundaries.

For each value of NB, the following formula is used to determine the problem size N, taking into account 80\% memory usage:

\[N = \left[\left(0.8 \sqrt{\frac{\text{Memory in GB} \times 1024^3}{8}}\right) \div NB\right] \times NB\]

The division by 8 in the inner parenthesis is the size in bytes of a double precision floating point number.


The online tool \href{http://hpl-calculator.sourceforge.net}{HPL Calculator} by Mohammad Sindi automates the process of calculating the problem size N for block sizes NB in the range 96 to 256, and for memory usage 80\% to 100\%.

The values of N determined using HPL Calculator were cross-checked with the formula above.

The processor grid dimensions P and Q represent a P x Q grid of processor cores. For example, the Aerin cluster has 8 nodes, each with 4 cores, giving a total of 32 processor cores. These core can be organised in compute grids of 1 x 32, 2 x 16 and 4 x 8.

The HPL algorithm favours P x Q grids as square as possible, i.e. with P almost equal to Q, but with P smaller than Q. So, for a single node with 4 cores, a processor grid of 1 x 4 gives better benchmark performance than 2 x 2.

If the Aerin Cluster used a high speed interconnect between nodes, such as InfiniBand, as used on large HPC clusters, maximum performance would be expected to be achieved using a processor grid of 4 x 8. This is the ``squarest'' possible P x Q grid using 32 cores whilst maintaining P less than Q. However, as noted in \href{https://www.netlib.org/benchmark/hpl/faqs.html}{HPL Frequently Asked Questions}, Ethernet is not a high speed interconnect. An Ethernet network is simplistically a single wire connecting the nodes, with each node competing (using random transmission times) for access to the wire to transmit data. This physical limitation reduces potential maximum cluster performance, and the maximum achievable performance is seen using a flatter P x Q grid. This proved to be the case, and maximum cluster performance was observed using a processor grid of 2 x 16 for all 8 nodes. This phenomena was also observed using when using less than 8 nodes.


%
% SUB SECTION
%
\subsection{Running HPL}

HPL uses the input file \verb|HPL.dat| to specify the input parameters for each benchmark run.

Each of these has a preceding count parameter (\verb|#|) which specifies the number of each parameter (for example, there may be more than 1 processor grid shape).

The problem size N and block size NB are set as follows, in this case there being a single problem size and a single block size:

\lstset{style=listing}
\begin{lstlisting}[numbers=none, caption=HPL.dat]
1            # of problems sizes (N)
52360        Ns
1            # of NBs
88           NBs
\end{lstlisting}

The processor grid shapre parameters P and Q are set as follows, in this case there being 32 cores/slots with 3 possible grid shapes, 1 x 32, 2 x 16 and 4 x 8:

\lstset{style=listing}
\begin{lstlisting}[numbers=none, caption=HPL.dat]
3            # of process grids (P x Q)
1 2 4        Ps
32 16 8      Qs
\end{lstlisting}

For each benchmark run, the above parameters are set accordingly.

For this project the remaining parameters, which have a lesser effect on benchmark results, were set in accordance with the advice in \href{https://www.netlib.org/benchmark/hpl/tuning.html}{HPL Tuning}, with \verb|swapping threshold| being set to match NB, as follows:

\lstset{style=listing}
\begin{lstlisting}[numbers=none, caption=HPL.dat]
16.0         threshold
1            # of panel fact
1            PFACTs (0=left, 1=Crout, 2=Right)
2            # of recursive stopping criterium
4 8          NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
2            RFACTs (0=left, 1=Crout, 2=Right)
2            # of broadcast
1 3          BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
2            # of lookahead depth
0 1          DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
88           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
\end{lstlisting}

HPL is run using a serial BLAS library as follows, in this case using 2 nodes with 4 cores/slots:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to core -host node1:4,node2:4 -np 8 xhpl
\end{lstlisting}

HPL is run using a multi-threaded BLAS library as follows, again, in this case using 2 nodes with 4 cores/slots:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to socket -host node1:1,node2:1 -np 2 -x OMP_NUM_THREADS=4 xhpl
\end{lstlisting}

The results generated by each benchmark run are either printed on \verb|stdout|, \verb|stderr|, or placed in a file, depending on the \verb|HPL.dat| parameter \verb|device out|.

For this project, all benchmark results were placed in a file called \verb|HPL.out| by specifying the filename and setting \verb|device out| to zero, as follows:

\lstset{style=listing}
\begin{lstlisting}[numbers=none, caption=HPL.dat]
HPL.out      output file name (if any)
0            device out (6=stdout,7=stderr,file)
\end{lstlisting}


The \verb|HPL.out| file from each benchmark run was renamed to reflect the N, NB, P and Q parameters used, and also the BLAS library used. For example:

\lstset{style=type}
\begin{lstlisting}
$ mv HPL.out HPL.out.6_node_45320_88_1_6_2_3.openblas_openmp
\end{lstlisting}

Each results file was then stored appropriately in the \verb|picluster/results| directory structure. 


%
% SECTION
%
\section{HPC Challenge (HPCC)}

HPCC is a suite of benchmarks which test different aspects of cluster performance. These benchmarks include tests for processing performance, memory bandwidth, and network bandwidth and latency. HPCC is intended to give a broader view of cluster performance than HPL alone, which should reflect real-world application performance more closely. HPCC includes HPL as one of the suite of benchmarks.

The HPCC suite consists of the following 7 benchmarks, where \emph{single} indicates the benchmark is run a single randomly selected node, \emph{star} indicates the benchmark in run independently on all nodes, and \emph{global} indicates the benchmark is run using all nodes in a coordinated manner. 


%
% SUB SECTION
%
\subsection{HPL}

HPL is a \emph{global} benchmark which solves a dense system of linear equations.


%
% SUB SECTION
%
\subsection{DGEMM}

The DGEMM benchmark tests double precision matrix-matrix multiplication performance in both \emph{single} and \emph{star} modes.

 
%
% SUB SECTION
%
\subsection{STREAM}

The STREAM benchmark tests memory bandwidth, to and from memory, in both \emph{single} and \emph{star} modes.


%
% SUB SECTION
%
\subsection{PTRANS}

PTRANS, Parallel Matrix Transpose, is a \emph{global} benchmark which tests system performance in transposing a large matrix.


%
% SUB SECTION
%
\subsection{RandomAccess}

The RandomAccess benchmark tests the performance of random updates to a large table in memory, in \emph{single}, \emph{star}, and \emph{global} modes.


%
% SUB SECTION
%
\subsection{FFT}

FFT tests the Fast Fourier Transform performance of a large vector, in \emph{single}, \emph{star}, and \emph{global} modes.


%
% SUB SECTION
%
\subsection{Network Bandwidth and Latency}

This benchmark measures network/communications bandwidth and latency in \emph{global} mode.


%
% SUB SECTION
%
\section{Running HPCC}

HPCC is run in the same manner as HPL. An input file \verb|hpccinf.txt| is created, which is of same format as \verb|HPL.dat|, but may contain additional problem sizes and block sizes for the PTRANS benchmark.

HPCC is run using a serial BLAS library as follows, in this case using 2 nodes with 4 cores/slots:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to core -host node1:4,node2:4 -np 8 hpcc
\end{lstlisting}

HPCC is run using a multi-threaded BLAS library as follows, again, in this case using 2 nodes with 4 cores/slots:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to socket -host node1:1,node2:1 -np 2 -x OMP_NUM_THREADS=4 hpcc
\end{lstlisting}

The benchmark results are placed in an output file \verb|hpccoutf.txt|.  

For this project each benchmark \verb|hpccoutf.txt| file was renamed to reflect the N, NB, P and Q parameters used, and also the BLAS library used. For example:

\lstset{style=type}
\begin{lstlisting}
$ mv hpccoutf.txt hpccoutf.txt.8_node_52400_200_1_8_2_4.blis_openmp
\end{lstlisting}

Each results file was then stored appropriately in the \verb|picluster/results| directory structure. 


%
% SECTION
%
\section{High Performance Conjugate Gradients (HPCG)}

HPCG is intended to be complementary to HPL, and to incentivise hardware manufacturers to improve computer architectures for modern HPC workloads. 

Quoting the Super Computing 2019 HPCG Handout:

\say{The HPC Conjugate Gradient (HPCG) benchmark uses a preconditioned conjugate gradient (PCG) algorithm to measure the performance of HPC platforms with respect to frequently observed, yet challenging, patterns of execution, memory access, and global communication.}

\say{The PCG implementation uses a regular 27-point stencil discretixation in 3 dimensions of an elliptic partial differential equation (PDE) with zero Dirichlet boundary condition. The 3-D domain is scaled to fill a 3-D virtual process grid of all available MPI process ranks. The CG iteration includes a local and symmetric Gauss-Seidel preconditioner, which computes a forward and a back solve with a triangular matrix. All of these features combined allow HPCG to deliver a more accurate performance metric for modern HPC architectures.}


%
% SUB SECTION
%
\subsection{Running HPCG}

In a similar manner to HPL and HPCC, HPCG uses an input file \verb|hpcg.dat| for benchmark run configuration.

The format of \verb|hpcg.dat| with default parameter values is:

\lstset{style=type}
\begin{lstlisting}
HPCG benchmark input file
Sandia National Laboratories; University of Tennessee, Knoxville
104 104 104
60
\end{lstlisting}

Lines 1 and 2 are comments, line 3 specifies the 3 dimensions of the problem size, and line 4 specifies the run time in seconds. The problem size is per OpenMPI process. For benchmark runs to be submitted for official performance ranking the problem size memory usage must exceed 25\% of available memory and the run time must exceed 30 minutes (1800 seconds).

HPCG can be built in serial and OpenMP versions. See Part II Chapter 10 for details.

For the serial version each node runs 4 \verb|xhpcg| processes, one on each core. Foe the OpenMP version a single \verb|xhpcg| process is run on each node.

The serial version of HPCG is run as follows, in this case on 2 nodes each with 4 cores:

\lstset{style=type}
\begin{lstlisting}
mpirun --bind-by core -host node1:4,node2:4 -np 8 xhpcg
\end{lstlisting}

The OpenMP version of HPCG is run as follows, again, in this case on 2 nodes each with 4 cores:

\lstset{style=type}
\begin{lstlisting}
mpirun --bind-by socket -host node1:1,node2:1 -np 2 -x OMP_NUM_THREADS=4 xhpcg
\end{lstlisting}

The output from each benchmark run is placed is an output file with a name including a timestamp, for example:

\verb|HPCG-Benchmark_3.1_2020-08-30_15-00-29.txt|


%
% SECTION
%
\section{BLAS Libraries}

If we use the Linux \verb|perf| command to sample and record CPU stack traces (via frame pointers) for an \verb|xhpl| process (with Process ID 6595) for 30 seconds:

\lstset{style=type}
\begin{lstlisting}
$ sudo perf record -p 6595 -g -- sleep 30
\end{lstlisting}

And then look at the stack trace report: 

\lstset{style=type}
\begin{lstlisting}
$ sudo perf report
\end{lstlisting}

\lstset{style=term}
\begin{lstlisting}
+ 100.00% 0.00% xhpl xhpl             [.] _start                                                                                        
+ 100.00% 0.00% xhpl libc-2.31.so     [.] __libc_start_main                                                                             
+ 100.00% 0.00% xhpl xhpl             [.] main                                                                                          
+ 100.00% 0.00% xhpl xhpl             [.] HPL_pdtest                                                                                    
+ 100.00% 0.00% xhpl xhpl             [.] HPL_pdgesv                                                                                    
+ 100.00% 0.00% xhpl xhpl             [.] HPL_pdgesv0                                                                                   
+  98.03% 0.00% xhpl xhpl             [.] HPL_pdupdateTT                                                                                
+  97.71% 0.00% xhpl libblas.so.3     [.] 0x0000ffffaa839ff0                                                                            
+  97.71% 0.00% xhpl libgomp.so.1.0.0 [.] GOMP_parallel                                                                                 
+  97.70% 0.00% xhpl libblas.so.3     [.] 0x0000ffffaa839e80                                                                            
-  96.56% 0.00% xhpl xhpl             [.] HPL_dgemm                                                                                     
    HPL_dgemm                                                                                                                                           
    dgemm_            
\end{lstlisting}

It can be seen that 96.56\% of the time within the \verb|xhpl| process is spent in the \verb|HPL_dgemm| function, which subsequently calls the BLAS \verb|dgemm_| function (the \verb|_| appended to \verb|dgemm| function name is the Fortran function name decoration).
 
It is for this reason that the efficiency of the BLAS library is critical for both benchmark and real-world application performance. The efficiency of the \verb|dgemm| (\textbf{d}ouble precision \textbf{ge}neral \textbf{m}atrix \textbf{m}ultiplication) function is particularly important for the dense matrix HPL benchmark.

The mathematical operation implemented by \verb|dgemm| is:

\[C := \alpha \times A \times B + \beta \times C\]

where $A$ is a $M \times K$ matrix, $B$ is a $K \times N$ matrix, $C$ is a $M \times N$ matrix, and $\alpha$ and $\beta$ are scalars.

In the case of the HPL benchmark, $M = N = K$.

Efficient BLAS libraries ``block'' matrix multiplication into smaller sub-matrix multiplications. The ``block'' sizes of these sub-matrices multiplications are carefully chosen to make optimum use of CPU registers, and L1, L2, and L3 (when available) cache sizes. These sub-matrix multiplications are referred to as \emph{kernels}, or sometimes \emph{micro-kernels}.

Maximum performance is achieved when the matrix multiplication problem size in an integer multiple of the \verb|dgemm| ``block'' size. In the case of the HPL benchmark, maximum performance is achieved when the the problem size N is an integer multiple of the HPL NB block size, which in turn is an integer multiple of the BLAS ``block'' size.

The above is depicted in Figure ??.

\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{dgemm.pdf}
	\caption{\texttt{dgemm} \emph{kernel} Matrix-Matrix Multiplication.}
	\label{fig:image2}
\end{figure}


%
% SECTION
%
\subsection{GotoBLAS}

GotoBLAS is a high performance BLAS library developed by Kazushige Goto at the Texas Advanced Computing Center (TACC), a department of the University of Texas at Austin.

GotoBLAS achieves high performance through the use of hand-crafted assembly language \emph{kernels}. Higher level BLAS routines are decomposed in \emph{kernels}, which stream data from the L1 and L2 CPU caches. These kernels typically reflect the size of the CPU registers, and L1 and L2 caches. For example, a CPU architecture may have a 4 x 4 \emph{dgemm kernel} and a 4 x 8 \emph{dgemm kernel} which conduct a double precision matrix-matrix multiplication on 4 x 4 and 4 x 8 matrices, respectively, and which have been sized for a specific architecture.

The source code for GotoBLAS and GotoBLAS2 is still available as Open Source software, but the library is no longer in active development.


%
% SECTION
%
\subsection{OpenBLAS}

OpenBLAS is an Open Source fork of the original GotoBLAS2 library, and is in active development by volunteers led by Zhang Xianyi at the Lab of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences (ISCAS).

OpenBLAS is used by many of the Top500 supercomputers, including the Fugaku supercomputer which tops the June 2020 TOP500 List.

For the Arm64 architecture, OpenBLAS implements the following \verb|dgemm| \emph{kernels}, where \verb|.S| indicates an assembly language file:

\begin{itemize}
  \item dgemm\_kernel\_4x4.S
  \item dgemm\_kernel\_4x8.S
  \item dgemm\_kernel\_8x4.S 
\end{itemize}


%
% SECTION
%
\subsection{BLIS}

The ``BLAS-like Library Instantiation Software'' (BLIS) is a BLAS library implementation for many CPU architectures, and also a framework for implementing new BLAS libraries for new architectures. Using the BLIS framework, by solely implementing an optimised \verb|dgemm| \emph{kernel} in assembly language or compiler intrinsics, BLAS library functionality can be realised which achieves 60\% - 90\% of theoretical maximum performance.

BLIS is developed by the Science of High-Performance Computing (SHPC) group of the Oden Institute for Computational Engineering and Sciences, at The University of Texas at Austin.

For the Arm64 architecture, BLIS implements the following \verb|dgemm| assembly language \emph{kernel}:

\begin{itemize}
  \item gemm\_armv8a\_asm\_6x8
\end{itemize}


%
% SECTION
%
\section{Pure OpenMPI Topology}

In a pure OpenMPI topology, work is distributed across the cluster nodes, and the processor cores on each node, by OpenMPI. Processor cores are referred to as \emph{slots}. The number of nodes in the cluster, and the number of slots on each node, are specified using either the \verb|-host| or \verb|-hostfile| parameter of the \verb|mpirun| command. Each processor slot is a target for a work process. The total number of work processes to be run is specified by the \verb|-np| parameter.

The \verb|-host| parameter is used to specify nodes and slots on the command line.

The \verb|-hostfile| parameter is used to specify a file which contains the nodes and slots information.

In the following example, the \verb|-host| parameter is used to specify 2 nodes, each with 4 slots, on which to run 8 \verb|xhpl| work processes:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to core -host node1:4,node2:4 -np 8 xhpl
\end{lstlisting}

The same number of nodes, slots and work processes is specified using the \verb|-hostfile| parameter as follows:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to core -hostfile nodes -np 8 xhpl
\end{lstlisting}

Where \verb|nodes| is a file containing the following:

\lstset{style=listing}
\begin{lstlisting}[numbers=none, caption=nodes]
node1 slots=4
node2 slots=4
\end{lstlisting}

The \verb|--bind-to core| parameter instructs \verb|mpirun| to not migrate a work processes from the core on which it was started. Once started on a specific core, a work process will remain \emph{bound} to that core. This is an optimisation which reduces cache refreshes when a work process is interrupted, by a kernel system call for example, and is then restarted.

A pure OpenMPI distribution of \verb|xhpl| work processes on a single node with 4 cores/slots is depicted in Figure ?? (a). Each \verb|xhpl| process calls the functions of a single-threaded BLAS serial library.


%
% SECTION
%
\section{Hybrid OpenMPI/OpenMP Topology}
In a hybrid OpenMPI/OpenMP topology, OpenMPI is used to distribute work between the nodes. Each node runs a single work process. OpenMP is then used to distribute the work of this single process between the node cores using a multi-threaded BLAS library.

The \verb|-host|, \verb|-hostfile| and \verb|-np| parameters of the \verb|mpirun| command are used in a same manner as the pure OpenMPI case, noting that each node now has 1 slot on which to run a work process.

The additional parameter \verb|-x| is required now required. This parameter distributes and sets the \emph{environmental variable} \verb|OMP_NUM_THREADS| on each node prior to a work process being started on each node. This variable is queried by the multi-threaded BLAS library, and the appropriate number of threads are utilized.

Using 2 nodes, and 4 cores per node, as per the pure OpenMPI example, 2 \verb|xhpl| processes are run, one on each node, with the multi-threaded BLAS library utilising 4 cores, as follows:

\lstset{style=type}
\begin{lstlisting}
$ mpirun --bind-to socket -host node1:1,node2:1 -np 2 -x OMP_NUM_THREADS=4 xhpl
\end{lstlisting}

The \verb|--bind-to socket| parameter indicates to \verb|mpirun| that the \verb|xhpl| process is not associated with a particular core, it is not to be \emph{bound} to a specific core. The OpenMP runtime will determine which core(s) are used to run the \verb|xhpl| process.

Note, without the \verb|--bind-to socket| parameter only a single thread will be utilised for a multi-threaded BLAS library, even if the \verb|OMP_NUM_THREADS| environmental variable is set correctly.

A hybrid OpenMPI/OpenMP distribution of work is depicted in Figure ?? (b). A single \verb|xhpl| process calls functions from a multi-threaded BLAS library which run as threads on the processor cores.

\begin{figure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.71\textwidth]{core-pure-openmpi.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.71\textwidth]{core-hybrid-openmpi-openmp.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{Single Node Toplologies.}
\label{fig:image2}
\end{figure}




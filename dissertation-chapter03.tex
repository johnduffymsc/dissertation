This chapter initially describes the \emph{matrix-matrix multiplication} operation, before describing the mathematical background to the High Performance Linpack (HPL) and High Performance Conjugate Gradients (HPCG) benchmarks.

The HPC Challenge (HPCC) benchmark suite is not described because the suite includes HPL and DGEMM which are already covered in this chapter.


%
% SECTION
%
\section{Matrix-Matrix Multiplication}

The \emph{matrix-matrix multiplication} operation is of fundamental importance in High Performance Computing. As indicated in Figure 3.1, when running HPL on all 8 nodes of the Aerin Cluster, 87.26\% of the run time is spent in the \verb|HPL_dgemm| function which calls the BLAS \verb|dgemm| function. The \verb|dgemm| function name is derived from \emph{\textbf{d}ouble precision \textbf{ge}neral \textbf{m}atrix \textbf{m}ultiplication}. Running on single node, without any networking overhead, this increases to 96+\%.

\begin{figure}[h]
	\centering	
	\includegraphics[width=1.0\textwidth]{screenshot-perf.png}
	\caption{Output of \texttt{sudo perf report} generated from recording the events of an \texttt{xhpl} process using \texttt{sudo perf record -p 9020 -g -- sleep 30}, where \texttt{9020} is the \texttt{xhpl} process identifier. As indicated, 87.26\% of the process time is spent in the \texttt{HPL\_dgemm} function, which calls the BLAS \texttt{dgemm} function.}
\end{figure}

The computational complexity of $\mathbf{C} = \mathbf{AB}$, where $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ $\in \mathbf{R}^{n \times n}$, using a naive algorithm is $\mathcal{O}(n^3)$. Each \emph{row-column} dot product is $\mathcal{O}(n)$, requiring $n$ multiplications and $n - 1$ additions. There are $n$ of these dot products per row of $\mathbf{A}$ and each column of $\mathbf{B}$, and $\mathbf{A}$ has $n$ rows. In the more general case of $\mathbf{A} \in \mathbf{R}^{m \times k}$ and $\mathbf{B} \in \mathbf{R}^{k \times n}$, the complexity is $\mathcal{O}(mkn).$ 

In 1969, Volker Strassen \cite{strassen} proved the computational complexity can be reduced to $\mathcal{O}(N^{\log _{2}7+o(1)})\approx \mathcal{O}(N^{2.8074})$. This is achieved by subdividing each matrix into sub-matrices of dimension $2 \times 2$. By doing so, and introducing intermediate matrices, it is possible to reduce the number of multiplications required from 8 to 7 for each sub-matrix multiplication. This is called the \emph{Strassen Algorithm}.

In 1990, Don Coppersmith and Shmuel Winograd \cite{coppersmith-winograd} further reduced the complexity to $\mathcal{O}(n^{2.375477})$. The \emph{Coppersmith-Winograd Algorithm} was further improved by Virginia Vassilevska Williams \cite{williams} in 2012, and most recently in 2014 by Fran√ßois Le Gall \cite{le-gall}, which reduced the complexity to $\mathcal {O}(n^{2.3728639})$.

However, the reductions in computational complexity beyond the naive algorithm also add implementation complexity, and potential numerical instability. The \emph{Strassen Algorithm}, for example, introduces \emph{subtraction} operations which are a source of rounding errors. And the benefits may not outweigh the implementation complexity unless the matrices are very large. For this reason the naive algorithm is usually implemented in practice.    

The $\mathcal{O}(n^3)$ complexity of \emph{matrix-matrix multiplication} plays a central role in HPC benchmarks because, compared to other matrix operations, it has the highest $\frac{\text{computation}}{\text{memory movement}}$ ratio. This tests data movement from main memory, through the L1/L2/L3 caches, to the processor registers for computational operations. In a computer cluster it also tests network connectivity. It is for this reason that BLAS libraries usually indicate performance, and comparison with alternative libraries, with plots of \verb|dgemm| Gflops versus matrix size.

\emph{Matrix-matrix multiplication} is a BLAS (Basic Linear Algebra Subroutines) Level 3 operation. BLAS libraries such as GotoBLAS (no longer under active development), and it's derivatives OpenBLAS and BLIS, achieve high performance \emph{matrix-matrix multiplication} through effective use of the L1/L2/L3 caches. The inherent looping over \emph{dot product} operations is carefully structured so that data is retained in the caches for maximum reuse.     

%
% SECTION
%
\section{High Performance Linpack (HPL)}

The HPL benchmark \cite{linpack-ppf} measures the time taken to solve a dense system of linear of equations, and from this timing derives a performance metric in Gflops. 

The system of equations is of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

Random data is generated to populate $\mathbf{A}$ and $\mathbf{b}$ for each benchmark run.

The system of equation is solved using \emph{LU factorisation with row partial pivoting}.

Before discussing the \emph{LU factorisation} in more detail it is worth noting that if the HPL algorithm is amended (as is permitted for a submission to the TOP500 List), then the \emph{Strassen} and \emph{Coppersmith-Winograd} algorithms discussed in the previous section for \emph{matrix-matrix multiplication} are excluded from being used. The amended algorithm must adhere to the floating point operation count for \emph{LU factorisation with row partial pivoting}, $2/3n^3 + \mathcal{O}(n^2)$ \cite{linpack-ppf}. This is to ensure uniformity across submissions. 

%
% SUB SECTION
%
\subsection{LU Factorisation}

\emph{LU Factorisation} is the splitting of a matrix $\mathbf{A}$ into an upper-triangular matrix $\mathbf{U}$, and a lower-triangular matrix $\mathbf{L}$, such that:

\begin{equation}
\mathbf{A}=\mathbf{L}\mathbf{U}
\end{equation}

The original system of equations (3.1) can then be restated as:

\begin{equation}
\mathbf{L}\mathbf{U}\mathbf{x}=\mathbf{b}
\end{equation}

This can be reformulated as two equations:

\begin{equation}
\mathbf{U}\mathbf{x}=\mathbf{y}
\end{equation}

\begin{equation}
\mathbf{L}\mathbf{y}=\mathbf{b}
\end{equation}

Because $\mathbf{L}$ is in lower-triangular form, (3.5) is is easily solved using forward substitution to yield $\mathbf{y}$. Subsequently, because $\mathbf{U}$ is in upper-triangular form, (3.4) is easily solved using backward substitution to yield $\mathbf{x}$. This solves the original system (3.1).


%
% SUB SECTION
%
\subsection{Block LU Factorisation}

A \emph{block algorithm} is an algorithm which operates on \emph{blocks} of data instead of scalar values \cite{blocklu}. In the case of \emph{block LU factorisation}, a matrix $\mathbf{A} \in \mathbf{R}^{n \times n}$ is partitioned into \emph{blocks} $\mathbf{A}_{ij} \in \mathbf{R}^{nb \times nb}$ and factorised into $\mathbf{L}_{ij}, \mathbf{U}_{ij} \in \mathbf{R}^{nb \times nb}$. 


\begin{align}
\mathbf{A} &= \begin{pmatrix}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23} \\
A_{31} & A_{32} & A_{33} 
\end{pmatrix}\\
&= \begin{pmatrix}
L_{11} & 0      & 0 \\
L_{21} & L_{22} & 0 \\
L_{31} & L_{32} & L_{33} 
\end{pmatrix}
\begin{pmatrix}
U_{11} & U_{12} & U_{13} \\
0      & U_{22} & U_{23} \\
0      & 0      & U_{33} 
\end{pmatrix}
\end{align}

Dense matrices are easily partitioned into \emph{blocks}, and the subsequent LU decomposition, which uses optimised BLAS Level 3 \emph{matrix-matrix multiplication}, can be distributed across multiple processes using MPI for parallel processing.


%
% SUB SECTION
%
\subsection{The HPL Algorithm}

As previously stated, HPL solves a system of equations of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

The HPL algorithm \cite{hpl-algorithm} perform LU factorisation of $\mathbf{A}$ with partial row pivoting. The $\mathbf{L}\mathbf{y}=\mathbf{b}$ forward substitution is conducted during the factorisation process. The $\mathbf{U}\mathbf{x}=\mathbf{y}$ backward subsitution is the final step. 

The matrix $\mathbf{A}$ is partitioned into \emph{blocks} of dimension $nb \times nb$, where $nb$ is the \emph{block size}.

For optimum performance the problem dimension $n$ should be selected to utilise all available memory, and be an integer multiple of $nb$. Equation (3.9) can be used to determine the problem dimension $n$ for a block size $nb$ and a required total memory usage.

\begin{equation}
n = \left[\left(memory\_percent \sqrt{\frac{memory\_in\_gb \times 1024^3}{8}}\right) \div nb\right] \times nb
\end{equation}

The division by 8 in the square root is the size in bytes of a double precision floating point number.

The system is solved by distributing the \emph{blocks} over a two dimensional $P \times Q$ grid of processors on a cluster of \emph{distributed memory} computers \cite{linpack-ppf}. MPI is used for \emph{block} distribution.

In a pure MPI topology, each $P \times Q$ grid element represents a processor core. In a hybrid OpenMPI/OpenMP topology, each grid element represents a cluster node, and OpenMP is used to distributes data between the cores.   





%
% SECTION
%
\section{High Performance Conjugate Gradients (HPCG)}

The motivation for developing the HPCG benchmark \cite{hpcg-benchmark} is that HPL is not a good indicator of the likely performance of computer systems running some modern HPC workloads. HPL is a good indicator for problems with a high computation to memory access pattern, which Dongarra, Heroux and Luszczek refer to as a \emph{Type 2} data access problem \cite{hpcg-benchmark}. But HPL is not a good indicator for problems with a low computation to memory access pattern, which Dongarra, Heroux and Luszczek refer to as \emph{Type 1} \cite{hpcg-benchmark}.

Type 1 data access patterns are typified by the solving of large sparse systems arising from the discretisation of partial differential equations. In the case of HPCG, the system is generated from the discretisation of the Poisson differential equation on a three dimensional grid using a 27-point stencil. 

The High Performance Conjugate Gradient (HPCG) benchmark \cite{hpcg-benchmark}, measures the time taken to solve such a system, and from this timing derives a performance metric in Gflops. The system of equations is of the form:

\begin{align}
\mathbf{Ax} = \mathbf{b},\text{ where } \mathbf{A} \text{ is a \emph{sparse} matrix}.
\end{align}

HPCG uses the Conjugate Gradient method with a symmetric Gauss-Seidel preconditioner to solve the system.


%
% SUB SECTION
%

\subsection{Sparse Matrices}

A sparse matrix is one in which most of the entries are zero. A sample sparse matrix from the SuiteSparse Matrix Collection \cite{sparsesuite} is depicted in Figure 3.2.  

\begin{figure}[]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{sparse-matrix-cvxbqp1.png}
		\caption{Sparse matrix \texttt{cvxbqp1}.}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.6\textwidth]{sparse-matrix-cvxbqp1-graph.png}
		\caption{Visualisation of sparse matrix \texttt{cvxbqp1}.}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{Sample Sparse Matrix.} Sparse matrix \texttt{cvxbqp1} generated from a quadratic programming optimisation problem. Authored by N Gould, and curated by the SuiteSparse Matrix Collection \cite{sparsesuite}. The matrix has 50,000 rows and 50,000 columns, with 349,968 non-zero entries.}
\label{fig:image2}
\end{figure}

It is not efficient to store the zero entries of a sparse matrix. Therefore, specific data formats have been developed to store sparse matrices, the most common being CSR (Compressed Sparse Row). The CSR format consists of three one-dimensional vectors:

\begin{itemize}
\item \textbf{V} which stores the matrix non-zero entries in row order.
\item \textbf{COL\_INDEX} which stores the column index of each entry in \textbf{V}.
\item \textbf{ROW\_INDEX} which stores the start index of each row in \textbf{V}.
\end{itemize}

A property of the CSR format is that it supports efficient \emph{matrix-vector multiplication}, which is important for iterative methods such as Conjugate Gradient.


%
%
%
\subsection{Krylov Subspace Methods}

A \emph{Krylov Subspace} is the subspace created by repeatedly applying a matrix $\mathbf{A}$ to a vector $\mathbf{v}$:

\begin{align}
\mathcal{K}_m(\mathbf{A}, \mathbf{v}) = \text{span\{}\mathbf{v}, \mathbf{A}\mathbf{v}, \mathbf{A}^2\mathbf{v}, ..., \mathbf{A}^{m-1}\mathbf{v}\text{\}}
\end{align}

A Krylov subspace method projects a higher dimensional problem into a lower dimensional Krylov Subspace.

%\begin{align}
%\mathcal{K}_m(\mathbf{A}, \mathbf{r}_0) = \text{span\{}\mathbf{r}_0, \mathbf{A}\mathbf{v}, \mathbf{A}%^2\mathbf{v}, ..., \mathbf{A}^{m-1}\mathbf{v}\text{\}}
%\end{align}


%
% SUB SECTION
%
\subsection{The Conjugate Gradients Method}

The Conjugate Gradient method is a Krylov Subspace method for solving a system of equations $\mathbf{A}\mathbf{x}=\mathbf{b}$, where $\mathbf{A}$ \emph{symmetric positive definite} matrix. It solves a $n \times n$ system in at most $n$ iterations.

At each step of the iteration the method minimises the A-Norm of the error.

The method is as follows \cite{cg-without-pain}:

\begin{align}
\mathbf{d}_0 &= \mathbf{r}_0 = \mathbf{b} - \mathbf{A}\mathbf{x}_0 \\
\alpha_i &= \frac{\mathbf{r}_i^{\mathbf{T}}\mathbf{r}_i}{\mathbf{d}_i^{\mathbf{T}}\mathbf{A}\mathbf{d}_i} \\
\mathbf{x}_{i+1} &= \mathbf{x}_i + \alpha_i \mathbf{d}_i \\
\mathbf{r}_{i+1} &= \mathbf{r}_i - \alpha_i \mathbf{A} \mathbf{d}_i \\
\beta_{i+1} &= \frac{\mathbf{r}_{i+1}^\mathbf{T} \mathbf{r}_{i+1}}{\mathbf{r}_{i}^\mathbf{T} \mathbf{r}_{i}} \\
\mathbf{d}_{i+1} &= \mathbf{r}_{i+1} + \beta_{i+1}\mathbf{d}_i
\end{align}

where at each iteration, $\mathbf{x}_i$ is the approximation to the solution, $\mathbf{r}_i$ is the residual, and $\mathbf{d}_i$ is the direction. 

At each iteration the method contains one \emph{matrix-vector multiplication}. Since, we are considering a sparse matrix $\mathbf{A}$, the computational complexity of this may be as low as $\mathcal{O}(n)$ \cite{numerical-linear-algebra}.


%
% SUB SECTION
%
\subsection{Parallel \emph{matrix-vector} Multiplication}

In section 3.3.1 it was stated that efficient \emph{matrix-vector multiplication} is a property of sparse matrices stored in CSR format. This is because for each \emph{dot-product} operation, the matrix column indices are used to index the required vector elements. This restricts the \emph{dot-product} to only non-zero multiplications.

Furthermore, it is possible to parallelise the \emph{matrix-vector} multiplication. Each \emph{dot-product} is independent and can be therefore be computed in parallel on the same node, or distributed to other nodes for processing.












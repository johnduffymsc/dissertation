This chapter initially discusses the \emph{matrix-matrix multiplication} operation, which is of fundamental importance in High Performance Computing, before discussing the mathematical background to the High Performance Linpack (HPL) and HP Conjugate Gradients (HPCG) benchmarks.


%
% SECTION
%
\section{Matrix-Matrix Multiplication}

The \emph{matrix-matrix multiplication} operation is of fundamental importance in HPC applications and benchmarks. As indicated in Figure 3.1, when running HPL on all 8 nodes of the Aerin Cluster, 87.26\% of the run time is spent in the \verb|HPL_dgemm| function which calls the BLAS \verb|dgemm_| function. The function name \verb|dgemm| is derived from \emph{\textbf{d}ouble precision \textbf{ge}neral \textbf{m}atrix \textbf{m}ultiplication}. Running on single node, without any networking overhead, this increases to 96+\%.

\begin{figure}[h]
	\centering	
	\includegraphics[width=1.0\textwidth]{screenshot-perf.png}
	\caption{Output of \texttt{sudo perf report} generated from recording the events of an \texttt{xhpl} process using \texttt{sudo perf record -p 9020 -g -- sleep 30}, where \texttt{9020} is the \texttt{xhpl} process identifier.}
\end{figure}

The naive computational complexity of $\mathbf{AB}$, where $\mathbf{A}$, $\mathbf{B}$ $\in \mathbf{R}^{n \times n}$, is $\mathcal{O}(n^3)$. Each \emph{row-column} dot product is $\mathcal{O}(n)$, requiring $n$ multiplications and $n - 1$ additions. There are $n$ of these dot products per row of $A$, and $A$ has $n$ rows. With $\mathbf{A} \in \mathbf{R}^{m \times k}$ and $\mathbf{B} \in \mathbf{R}^{k \times n}$, the complexity is $\mathcal{O}(mkn).$ 

In 1969, Volker Strassen proved the computational complexity can be reduced to $\mathcal{O}(N^{\log _{2}7+o(1)})\approx \mathcal{O}(N^{2.8074})$. And in 1990, Don Coppersmith and Shmuel Winograd further reduced the complexity to $\mathcal{O}(n^{2.375477})$. The most recent bound on complexity was by Fran√ßois Le Gallf in 2014, which reduced the complexity to $\mathcal {O}(n^{2.3728639})$.

However, these reductions in mathematical complexity also add computational implementation complexity. The benefits may not outweigh the computational overhead unless the matrices are very large. For this reason the naive algorithm is often the implementation of choice.    

The $\mathcal{O}(n^3)$ complexity of \emph{matrix-matrix multiplication} plays a central role in HPC benchmarks because, compared to other matrix operations, it has the highest $\frac{\text{computation}}{\text{memory movement}}$ ratio. This stresses and tests data movement from main memory, through the L1/L2/L3 caches, to the processor registers for computational operations. In a computer cluster it also stresses and tests network connectivity.



%
% SECTION
%
\section{High Performance Linpack (HPL)}


%
% SECTION
%
\section{HP Conjugate Gradients (HPCG)}




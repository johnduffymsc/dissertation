This chapter initially decribes the \emph{matrix-matrix multiplication} operation, before describing the mathematical background to the High Performance Linpack (HPL) and HP Conjugate Gradients (HPCG) benchmarks.


%
% SECTION
%
\section{Matrix-Matrix Multiplication}

The \emph{matrix-matrix multiplication} operation is of fundamental importance in High Performance Computing. As indicated in Figure 3.1, when running HPL on all 8 nodes of the Aerin Cluster, 87.26\% of the run time is spent in the \verb|HPL_dgemm| function which calls the BLAS \verb|dgemm| function. The \verb|dgemm| function name is derived from \emph{\textbf{d}ouble precision \textbf{ge}neral \textbf{m}atrix \textbf{m}ultiplication}. Running on single node, without any networking overhead, this increases to 96+\%.

\begin{figure}[h]
	\centering	
	\includegraphics[width=1.0\textwidth]{screenshot-perf.png}
	\caption{Output of \texttt{sudo perf report} generated from recording the events of an \texttt{xhpl} process using \texttt{sudo perf record -p 9020 -g -- sleep 30}, where \texttt{9020} is the \texttt{xhpl} process identifier.}
\end{figure}

The naive computational complexity of $\mathbf{C} = \mathbf{AB}$, where $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ $\in \mathbf{R}^{n \times n}$, is $\mathcal{O}(n^3)$. Each \emph{row-column} dot product is $\mathcal{O}(n)$, requiring $n$ multiplications and $n - 1$ additions. There are $n$ of these dot products per row of $\mathbf{A}$ and each column of $\mathbf{B}$, and $\mathbf{A}$ has $n$ rows. In the more general case of $\mathbf{A} \in \mathbf{R}^{m \times k}$ and $\mathbf{B} \in \mathbf{R}^{k \times n}$, the complexity is $\mathcal{O}(mkn).$ 

In 1969, Volker Strassen proved the computational complexity can be reduced to $\mathcal{O}(N^{\log _{2}7+o(1)})\approx \mathcal{O}(N^{2.8074})$. This is achieved by subdividing each matrix into sub-matrices of dimension $2 \times 2$. By doing do, and introducing intermediate matrices, it is possible to reduce the number of multiplications required from 8 to 7 for each sub-matrix multiplication. This is called the \emph{Strassen Algorithm}.

In 1990, Don Coppersmith and Shmuel Winograd further reduced the complexity to $\mathcal{O}(n^{2.375477})$. The \emph{Coppersmith-Winograd Algorithm} was further improved by Andrew Stothers and Virginia Vassilevska Williams, and most recently in 2014 by Fran√ßois Le Gallf, which reduced the complexity to $\mathcal {O}(n^{2.3728639})$.

However, the reductions in complexity beyond the \emph{Strassen Algorithm} also add implementation complexity. The benefits may not outweigh the computational overhead unless the matrices are very large. For this reason the naive and \emph{Strassen Algorithm} are usually implemented in practice.    

The $\mathcal{O}(n^3)$ complexity of \emph{matrix-matrix multiplication} plays a central role in HPC benchmarks because, compared to other matrix operations, it has the highest $\frac{\text{computation}}{\text{memory movement}}$ ratio. This tests data movement from main memory, through the L1/L2/L3 caches, to the processor registers for computational operations. In a computer cluster it also tests network connectivity. It is for this reason that BLAS libraries usually indicate performance, and comparison with alternative libraries, with plots of \verb|dgemm| Gflops versus matrix size.


%
% SECTION
%
\section{High Performance Linpack (HPL)}

TODO CITE: "The LINPACK Benchmark: past, present and future", Dongarra, Luszczek \& Petitet

The High Performance Linpack (HPL) benchmark measures the time taken to solve a dense system of linear of equations, and from this timing derives a Gflops performance metric. 

The system of equations is of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

Random data is generated to populate $\mathbf{A}$ and $\mathbf{b}$ for each benchmark run.

The system of equation is solved using \emph{LU factorisation with row partial pivoting}.

Before discussing the \emph{LU factorisation} in more detail it is worth noting that if the HPL algorithm is amended (as is permitted for a submission to the TOP500 List), then the \emph{Strassen} and \emph{Coppersmith-Winograd} algorithms discussed in the previous section for \emph{matrix-matrix multiplication} are excluded from being used. The amended algorithm must adhere to the floating point operation count for \emph{LU factorisation with row partial pivoting}, $2/3n^3 + \mathcal{O}(n^2)$. This is to ensure uniformity across submissions. 

%
% SUB SECTION
%
\subsection{LU Factorisation}

TODO CITE: "Numerical Linear Algebra", Trefethen \& Bau

\emph{LU Factorisation} is the splitting of a matrix $\mathbf{A}$ into an upper-triangular matrix $\mathbf{U}$, and a lower-triangular matrix $\mathbf{L}$, such that:

\begin{equation}
\mathbf{A}=\mathbf{L}\mathbf{U}
\end{equation}

The original system of equations (3.1) can then be restated as:

\begin{equation}
\mathbf{L}\mathbf{U}\mathbf{x}=\mathbf{b}
\end{equation}

This can be reformulated as two equations:

\begin{equation}
\mathbf{U}\mathbf{x}=\mathbf{y}
\end{equation}

\begin{equation}
\mathbf{L}\mathbf{y}=\mathbf{b}
\end{equation}

Because $\mathbf{L}$ is in lower-triangular form, (3.5) is is easily solved using forward substitution to yield $\mathbf{y}$. Subsequently, because $\mathbf{U}$ is in upper-triangular form, (3.4) is easily solved using backward substitution to yield $\mathbf{x}$. This solves the original system (3.1).

It remains to show how $\mathbf{A}$ is factored into $\mathbf{L}$ and $\mathbf{U}$.

The matrix $\mathbf{A}$ can be factored into an upper-diagonal matrix $\mathbf{U}$ in a similar manner to \emph{Gaussian Elimination}. Multiples of each row are subtracted from subsequent rows to produce \emph{zeros} below the matrix diagonal. Each of these subtraction operations can be considered a matrix operation $\mathbf{L}_i$ on $\mathbf{A}$. For example:

\begin{equation}
\begin{split}
\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
x & x & x & x \\
x & x & x & x \\
x & x & x & x
\end{pmatrix} \\
\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & x & x & x \\
0 & x & x & x
\end{pmatrix} \\
\mathbf{L}_2\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & x & x
\end{pmatrix} \\
\mathbf{L}_3\mathbf{L}_2\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & 0 & x
\end{pmatrix} \\
\mathbf{L}_3\mathbf{L}_2\mathbf{L}_1\mathbf{A} &= \mathbf{U}
\end{split}
\end{equation}

And so,

\begin{equation}
\mathbf{A} = \mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\mathbf{L}_3^{-1}\mathbf{U} = \mathbf{L}\mathbf{U}
\end{equation}

The product $\mathbf{L} = \mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\mathbf{L}_3^{-1}$ is, surprisingly, easily determined because each inverse $\mathbf{L}_i^{-1}$ is equal to $\mathbf{L}_i$ with its values below the diagonal negated, and the columns of the inverses then map directly into the same locations in $\mathbf{L}$. ``Trefethen and Bau" refers to these as two \emph{Stokes of Luck}, which are explained by the sparsity nature of the matrices $\mathbf{L}_i$. 


%
% SUB SECTION
%
\subsection{Row Partial Pivoting}

\emph{Row partial pivoting} is the swapping of rows during the factorisation of $\mathbf{A}$ into the upper-diagonal matrix $\mathbf{U}$. At each step of the factorisation, rows are swapped such that the \emph{pivot} row has the highest numerical value at the \emph{pivot point}. The reason for doing this is twofold. Firstly, if the value of the pivot point is \emph{zero}, then the factorisation fails. And secondly, to improve numerical stability CITE: Trefethen \& Bau.

For example, initially $a_{11}$ is the \emph{pivot point}. However $a_{41}$ has the highest numerical value in the \emph{pivot column}. So, $row_1$ and $row_4$ are swapped (pivoted) by the application of matrix $P_1$:

\begin{equation}
\begin{split}
\mathbf{A} &=
\begin{pmatrix}
1 & 2 & 3 & 4 \\
2 & 5 & 8 & 11 \\
4 & 10 & 14 & 16 \\
4 & 8 & 12 & 20 
\end{pmatrix} \\
\mathbf{P}_1\mathbf{A} &=
\begin{pmatrix}
4 & 8 & 12 & 20 \\
2 & 5 &  8 & 11 \\
4 & 10 & 14 & 16 \\
1 & 2 & 3 & 4
\end{pmatrix} \\
\end{split}
\end{equation}

The factorisation matrix $\mathbf{L}_1$ is then applied as before:
 
\begin{equation}
\begin{split}
\mathbf{L}_1\mathbf{P}_1\mathbf{A} =
\begin{pmatrix}
4 & 8 & 12 & 14 \\
0 & 1 & 2 & 1 \\
0 & 2 & 2 & -4 \\
0 & 0 & 0 & -1
\end{pmatrix} \\
\end{split}
\end{equation}

The final factorisation is of the form:

\begin{equation}
\mathbf{L}_3\mathbf{P}_3\mathbf{L}_2\mathbf{P}_2\mathbf{L}_1\mathbf{P}_1\mathbf{A} = \mathbf{U}
\end{equation}

By what ``Trefethen and Bau'' refer to as a third \emph{Stroke of Luck}, (3.10) can be restated, and easily determined, as:

\begin{equation}
\mathbf{P}\mathbf{A} = \mathbf{L}\mathbf{U}
\end{equation}

where

\begin{equation}
\mathbf{L} = (\mathbf{L}_3^{'}\mathbf{L}_2^{'}\mathbf{L}_1^{'})^{-1} \text{, and } \mathbf{P} = \mathbf{P}_3\mathbf{P}_2\mathbf{P}_1
\end{equation}

and where $\mathbf{L}_i^{'} = \mathbf{L}_i$ with the entries below the diagonal permuted. TODO: Expand upon?


%
% SUB SECTION
%
\subsection{The HPL Algorithm}

TODO CITE: "The LINPACK Benchmark: past, present and future", Dongarra, Luszczek \& Petitet

TODO CITE: "HPL Algorithm", NetLib

As previously stated, HPL solves a system of equations of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

The system is solved by distributing the data over a two dimensional grid of processors, $P \times Q$, on a cluster of \emph{distributed memory} computers. HPL requires an implementation of the Message Passing Interface (MPI) for the data distribution. The Aerin Cluster uses the OpenMPI implementation. In a pure MPI topology, each grid element represents a processor core of a CPU. In a hybrid OpenMPI/OpenMP topology, each grid element represents a cluster node, and OpenMP distributes data between the cores of the node.   

HPL partitions the data into \emph{blocks} of dimension $NB \times NB$, where $NB$ is referred to as the \emph{block size}. The data blocks are distributed cyclically between the elements of the processor grid for load balancing and scalability. 

For optimum performance the problem dimension $n$ should be selected to utilise all available memory, and be integer multiple of the block size $NB$. Equation (3.14) can be used to determine the problem dimension N for a given block size $NB$ and a required total memory usage.

\begin{equation}
N = \left[\left(memory\_percent \sqrt{\frac{memory\_in\_gb \times 1024^3}{8}}\right) \div NB\right] \times NB
\end{equation}

The division by 8 in the inner parenthesis is the size in bytes of a double precision floating point number.

The LU factorisation is logically partitioned using the same block size $NB$ used for data distribution.


%
% SECTION
%
\section{HP Conjugate Gradients (HPCG)}

TODO CITE: "",

The motivation for creating 




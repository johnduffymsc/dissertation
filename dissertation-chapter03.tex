This chapter initially describes the \emph{matrix-matrix multiplication} operation, before describing the mathematical background to the High Performance Linpack (HPL) and High Performance Conjugate Gradients (HPCG) benchmarks.

The HPC Challenge (HPCC) benchmark suite is not described because the suite includes HPL and DGEMM which are already covered in this chapter.


%
% SECTION
%
\section{Matrix-Matrix Multiplication}

The \emph{matrix-matrix multiplication} operation is of fundamental importance in High Performance Computing. As indicated in Figure 3.1, when running HPL on all 8 nodes of the Aerin Cluster, 87.26\% of the run time is spent in the \verb|HPL_dgemm| function which calls the BLAS \verb|dgemm| function. The \verb|dgemm| function name is derived from \emph{\textbf{d}ouble precision \textbf{ge}neral \textbf{m}atrix \textbf{m}ultiplication}. Running on single node, without any networking overhead, this increases to 96+\%.

\begin{figure}[h]
	\centering	
	\includegraphics[width=1.0\textwidth]{screenshot-perf.png}
	\caption{Output of \texttt{sudo perf report} generated from recording the events of an \texttt{xhpl} process using \texttt{sudo perf record -p 9020 -g -- sleep 30}, where \texttt{9020} is the \texttt{xhpl} process identifier. As indicated, 87.26\% of the process time is spent in the \texttt{HPL\_dgemm} function, which calls the BLAS \texttt{dgemm} function.}
\end{figure}

The naive computational complexity of $\mathbf{C} = \mathbf{AB}$, where $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ $\in \mathbf{R}^{n \times n}$, is $\mathcal{O}(n^3)$. Each \emph{row-column} dot product is $\mathcal{O}(n)$, requiring $n$ multiplications and $n - 1$ additions. There are $n$ of these dot products per row of $\mathbf{A}$ and each column of $\mathbf{B}$, and $\mathbf{A}$ has $n$ rows. In the more general case of $\mathbf{A} \in \mathbf{R}^{m \times k}$ and $\mathbf{B} \in \mathbf{R}^{k \times n}$, the complexity is $\mathcal{O}(mkn).$ 

In 1969, Volker Strassen \cite{strassen} proved the computational complexity can be reduced to $\mathcal{O}(N^{\log _{2}7+o(1)})\approx \mathcal{O}(N^{2.8074})$. This is achieved by subdividing each matrix into sub-matrices of dimension $2 \times 2$. By doing so, and introducing intermediate matrices, it is possible to reduce the number of multiplications required from 8 to 7 for each sub-matrix multiplication. This is called the \emph{Strassen Algorithm}.

In 1990, Don Coppersmith and Shmuel Winograd \cite{coppersmith-winograd} further reduced the complexity to $\mathcal{O}(n^{2.375477})$. The \emph{Coppersmith-Winograd Algorithm} was further improved by Virginia Vassilevska Williams \cite{williams} in 2012, and most recently in 2014 by Fran√ßois Le Gallf \cite{le-gall}, which reduced the complexity to $\mathcal {O}(n^{2.3728639})$.

However, the reductions in complexity beyond the \emph{Strassen Algorithm} also add implementation complexity. The benefits may not outweigh the computational overhead unless the matrices are very large. For this reason the naive and \emph{Strassen Algorithm} are usually implemented in practice.    

The $\mathcal{O}(n^3)$ complexity of \emph{matrix-matrix multiplication} plays a central role in HPC benchmarks because, compared to other matrix operations, it has the highest $\frac{\text{computation}}{\text{memory movement}}$ ratio. This tests data movement from main memory, through the L1/L2/L3 caches, to the processor registers for computational operations. In a computer cluster it also tests network connectivity. It is for this reason that BLAS libraries usually indicate performance, and comparison with alternative libraries, with plots of \verb|dgemm| Gflops versus matrix size.


%
% SECTION
%
\section{High Performance Linpack (HPL)}

The High Performance Linpack (HPL) Benchmark \cite{linpack-ppf} measures the time taken to solve a dense system of linear of equations, and from this timing derives a performance metric stated in Gflops. 

The system of equations is of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

Random data is generated to populate $\mathbf{A}$ and $\mathbf{b}$ for each benchmark run.

The system of equation is solved using \emph{LU factorisation with row partial pivoting}.

Before discussing the \emph{LU factorisation} in more detail it is worth noting that if the HPL algorithm is amended (as is permitted for a submission to the TOP500 List), then the \emph{Strassen} and \emph{Coppersmith-Winograd} algorithms discussed in the previous section for \emph{matrix-matrix multiplication} are excluded from being used. The amended algorithm must adhere to the floating point operation count for \emph{LU factorisation with row partial pivoting}, $2/3n^3 + \mathcal{O}(n^2)$ \cite{linpack-ppf}. This is to ensure uniformity across submissions. 

%
% SUB SECTION
%
\subsection{LU Factorisation}

\emph{LU Factorisation} is the splitting of a matrix $\mathbf{A}$ into an upper-triangular matrix $\mathbf{U}$, and a lower-triangular matrix $\mathbf{L}$, such that:

\begin{equation}
\mathbf{A}=\mathbf{L}\mathbf{U}
\end{equation}

The original system of equations (3.1) can then be restated as:

\begin{equation}
\mathbf{L}\mathbf{U}\mathbf{x}=\mathbf{b}
\end{equation}

This can be reformulated as two equations:

\begin{equation}
\mathbf{U}\mathbf{x}=\mathbf{y}
\end{equation}

\begin{equation}
\mathbf{L}\mathbf{y}=\mathbf{b}
\end{equation}

Because $\mathbf{L}$ is in lower-triangular form, (3.5) is is easily solved using forward substitution to yield $\mathbf{y}$. Subsequently, because $\mathbf{U}$ is in upper-triangular form, (3.4) is easily solved using backward substitution to yield $\mathbf{x}$. This solves the original system (3.1).

It remains to show how $\mathbf{A}$ is factored into $\mathbf{L}$ and $\mathbf{U}$.

The matrix $\mathbf{A}$ can be factored into an upper-diagonal matrix $\mathbf{U}$ in a similar manner to \emph{Gaussian Elimination}. Multiples of each row are subtracted from subsequent rows to produce \emph{zeros} below the matrix diagonal. Each of these subtraction operations can be considered a matrix operation $\mathbf{L}_i$ on $\mathbf{A}$. For example:

\begin{equation}
\begin{split}
\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
x & x & x & x \\
x & x & x & x \\
x & x & x & x
\end{pmatrix} \\
\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & x & x & x \\
0 & x & x & x
\end{pmatrix} \\
\mathbf{L}_2\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & x & x
\end{pmatrix} \\
\mathbf{L}_3\mathbf{L}_2\mathbf{L}_1\mathbf{A} &=
\begin{pmatrix}
x & x & x & x \\
0 & x & x & x \\
0 & 0 & x & x \\
0 & 0 & 0 & x
\end{pmatrix} \\
\mathbf{L}_3\mathbf{L}_2\mathbf{L}_1\mathbf{A} &= \mathbf{U}
\end{split}
\end{equation}

And so,

\begin{equation}
\mathbf{A} = \mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\mathbf{L}_3^{-1}\mathbf{U} = \mathbf{L}\mathbf{U}
\end{equation}

The product $\mathbf{L} = \mathbf{L}_1^{-1}\mathbf{L}_2^{-1}\mathbf{L}_3^{-1}$ is, surprisingly, easily determined because each inverse $\mathbf{L}_i^{-1}$ is equal to $\mathbf{L}_i$ with its values below the diagonal negated, and the columns of the inverses then map directly into the same locations in $\mathbf{L}$. ``Trefethen and Bau" \cite{numerical-linear-algebra} refers to these as two \emph{Stokes of Luck}, which are explained by the sparsity nature of the matrices $\mathbf{L}_i$. 


%
% SUB SECTION
%
\subsection{Row Partial Pivoting}

\emph{Row partial pivoting} is the swapping of rows during the factorisation of $\mathbf{A}$ into the upper-diagonal matrix $\mathbf{U}$. At each step of the factorisation, rows are swapped such that the \emph{pivot} row has the highest numerical value at the \emph{pivot point}. The reason for doing this is twofold. Firstly, if the value of the pivot point is \emph{zero}, then the factorisation fails. And secondly, to improve numerical stability \cite{numerical-linear-algebra}.

For example, initially $a_{11}$ is the \emph{pivot point}. However $a_{41}$ has the highest numerical value in the \emph{pivot column}. So, $row_1$ and $row_4$ are swapped (pivoted) by the application of matrix $P_1$:

\begin{equation}
\begin{split}
\mathbf{A} &=
\begin{pmatrix}
1 & 2 & 3 & 4 \\
2 & 5 & 8 & 11 \\
4 & 10 & 14 & 16 \\
4 & 8 & 12 & 20 
\end{pmatrix} \\
\mathbf{P}_1\mathbf{A} &=
\begin{pmatrix}
4 & 8 & 12 & 20 \\
2 & 5 &  8 & 11 \\
4 & 10 & 14 & 16 \\
1 & 2 & 3 & 4
\end{pmatrix} \\
\end{split}
\end{equation}

The factorisation matrix $\mathbf{L}_1$ is then applied as before:
 
\begin{equation}
\begin{split}
\mathbf{L}_1\mathbf{P}_1\mathbf{A} =
\begin{pmatrix}
4 & 8 & 12 & 14 \\
0 & 1 & 2 & 1 \\
0 & 2 & 2 & -4 \\
0 & 0 & 0 & -1
\end{pmatrix} \\
\end{split}
\end{equation}

The final factorisation is of the form:

\begin{equation}
\mathbf{L}_3\mathbf{P}_3\mathbf{L}_2\mathbf{P}_2\mathbf{L}_1\mathbf{P}_1\mathbf{A} = \mathbf{U}
\end{equation}

By what ``Trefethen and Bau'' \cite{numerical-linear-algebra} refer to as a third \emph{Stroke of Luck}, (3.10) can be restated, and easily determined, as:

\begin{equation}
\mathbf{P}\mathbf{A} = \mathbf{L}\mathbf{U}
\end{equation}

where

\begin{equation}
\mathbf{L} = (\mathbf{L}_3^{'}\mathbf{L}_2^{'}\mathbf{L}_1^{'})^{-1} \text{, and } \mathbf{P} = \mathbf{P}_3\mathbf{P}_2\mathbf{P}_1
\end{equation}

and where $\mathbf{L}_i^{'} = \mathbf{L}_i$ with the entries below the diagonal permuted.


%
% SUB SECTION
%
\subsection{The HPL Algorithm}

As previously stated, HPL solves a system of equations of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

The system is solved by distributing the data over a two dimensional grid of processors, $P \times Q$, on a cluster of \emph{distributed memory} computers \cite{linpack-ppf}. HPL requires an implementation of the Message Passing Interface (MPI) for the data distribution. The Aerin Cluster uses the OpenMPI implementation. In a pure MPI topology, each grid element represents a processor core of a CPU. In a hybrid OpenMPI/OpenMP topology, each grid element represents a cluster node, and OpenMP distributes data between the cores of the node.   

HPL partitions the data into \emph{blocks} of dimension $NB \times NB$, where $NB$ is referred to as the \emph{block size}. The data blocks are distributed cyclically between the elements of the processor grid for load balancing and scalability. 

For optimum performance the problem dimension $n$ should be selected to utilise all available memory, and be integer multiple of the block size $NB$. Equation (3.14) can be used to determine the problem dimension N for a given block size $NB$ and a required total memory usage.

\begin{equation}
N = \left[\left(memory\_percent \sqrt{\frac{memory\_in\_gb \times 1024^3}{8}}\right) \div NB\right] \times NB
\end{equation}

The division by 8 in the square root is the size in bytes of a double precision floating point number.

The LU factorisation is logically partitioned using the same block size $NB$ used for data distribution.


%
% SECTION
%
\section{High Performance Conjugate Gradients (HPCG)}

Like HPL, the HPCG benchmark, developed by Dongarra, Heroux and Luszczek \cite{hpcg-benchmark}, measures the time taken to solve a system of linear equations. And from this timing derives a performance metric stated in Gflops. The system of equations is also of the form:

\begin{equation}
\mathbf{Ax} = \mathbf{b},\text{ where }\mathbf{A} \in \mathbf{R}^{n\times n}\text{ and }\mathbf{x}, \mathbf{b} \in \mathbf{R}^n
\end{equation} 

This is where the similarity with HPL ends.

The HPCG system of equations is a sparse system resulting from a discretised three dimensional partial differential equation model. The benchmark computes \emph{preconditioned Conjugate Gradient} iterations for the sparse system.

The motivation for developing HPCG is that HPL no longer accurately indicates the likely performance of computer systems running modern HPC workloads. Historically, solving dense linear systems was the mainstay of HPC computing. This workload was predominantly the \emph{matrix-matrix multiplication} of locally aligned data, which was closely correlated with HPL benchmark preformance. This data access pattern, which represents a high computation to memory access ratio, is termed \emph{Type 2} by Dongarra, Heroux and Luszczek. HPCG address the need to measure the performance of \emph{distributed memory} computer systems when running a workload with a low computation to memory access pattern, which Dongarra, Heroux and Luszczek refer to as \emph{Type 1}. This is typified by the solving of sparse systems resulting from partial differential equations.

HPCG uses the Conjugate Gradient method with a symmetric Gauss-Seidel preconditioner to solve the Poisson differential equation on a three dimensional grid discretised with a 27-point stencil \cite{hpcg-benchmark}.

The Conjugate Gradient Method is a method for solving a system of equations with a \emph{symmetric positive-definite} matrix. When implemented as an iterative scheme, it solves a $n \times n$ system in at most $n$ iterations. The method will be introduced through a discussion of \emph{Quadratic Forms}, the \emph{Steepest Descent Method} and the \emph{Method of Conjugate Directions}. This broadly follows the description of the method by Shewchuck \cite{cg-without-pain}.

And finally... discuss the blocking/implementation of HPCG... and why CG is used. 

%
% SUB SECTION
%
\subsection{Quadratic Forms and the Relationship to $\mathbf{A}\mathbf{x}=\mathbf{b}$}

A \emph{quadratic form} is a quadratic function of a vector $\mathbf{x}$, whose return value is a scalar. It is of the form:

\begin{equation}
f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^{\mathbf{T}}\mathbf{A}\mathbf{x} - \mathbf{x}^{\mathbf{T}}\mathbf{b} + c
\end{equation}

where $\mathbf{A}$ is a matrix, $\mathbf{b}$ is a vector and $c$ is a scalar.

Consider the derivatives of (3.16):

\begin{equation}
\begin{split}
\nabla f(\mathbf{x}) &= \mathbf{A}\mathbf{x} - \mathbf{b} \\
\nabla^2 f(\mathbf{x}) &= \mathbf{A}
\end{split}
\end{equation}

Equation (3.16) is minimised when $\nabla f(\mathbf{x}) = 0$, i.e. $\mathbf{A}\mathbf{x} = \mathbf{b}$, and $\mathbf{A}$ is symmetric positive-definite.

So, finding the minimum of the quadratic form by an iterative method is equivalent to finding the solution of $\mathbf{A}\mathbf{x} = \mathbf{b}$ by a direct method. And, if a sparse system occupies all of the computer system memory, it will not be possible to solve the system by a direct method.


%
% SUB SECTION
%
\subsection{Steepest Descent}

The \emph{Steepest Descent} method is an iterative method for determining the minimum $\mathbf{x}$ of an \emph{objective function}. The name of the method is derived from the fact that at each iteration, the next step is taken in the direction of steepest descent.

For example, starting at an arbitrary point $\mathbf{x}_0$, the first iteration generates the first approximation $\mathbf{x}_1$ of the exact (unknown) solution $\mathbf{x}$ as:

\begin{equation}
\mathbf{x}_1=\mathbf{x}_0 + \alpha_0 \mathbf{r}_0
\end{equation}

where $\mathbf{r}_0$ is the step direction and $\alpha_0$ is the step length.

From (3.17) it can be seen that at each iteration the direction of steepest descent is:

\begin{equation}
\mathbf{r}_i = -\nabla f(\mathbf{x_i}) = \mathbf{b} - \mathbf{A}\mathbf{x_i}
\end{equation}

The step direction $\mathbf{r}_i$ is also termed the \emph{residual} and measures the difference between $\mathbf{b}$ the current value of $\mathbf{A}\mathbf{x}_i$.

The step length $\alpha_i$ is chosen such that the directional derivative along the direction of steepest descent is \emph{zero}. This occurs when $\nabla f(\mathbf{x_{i+1}})^\mathbf{T}\mathbf{r_i} = 0$, i.e. when they are orthogonal. The result of this is that \emph{steepest descent} potentially takes many steps in the same direction as the iteration progresses towards the exact solution. This leads on to the \emph{conjugate directions} method which attempts to address this.


%
% SUB SECTION
%
\subsection{Conjugate Directions}



%
% SUB SECTION
%
\subsection{Conjugate Gradients}



%
% SUB SECTION
%
\subsection{Why use Conjugate }










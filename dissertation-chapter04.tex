This chapter describes the hardware and software components of the Aerin Cluster. A description of the BLAS (Basic Linear Algebra Subroutines) libraries installed on the cluster is also included, together with a description of pure OpenMPI and hybrid OpenMPI/OpenMP cluster topologies. Finally, a description of Pi Cluster Tools, a suite of scripts which make command line management of the cluster easier and less prone to error is also included. 

Detailed build instructions for the Aerin Cluster, and the implementation details of Pi Cluster Tools, are included in the \verb|picluster| repository wiki. 

\begin{figure}[h]
	\centering	
	\includegraphics[width=0.95\textwidth]{aerin.jpg}
	\caption{\textbf{The Aerin Cluster}.}
\end{figure}



%
% SECTION
%
\section{Hardware}

The Aerin Cluster consists of the following hardware components:

\begin{itemize}
  \item 8 x Raspberry Pi 4 Model B compute nodes, \verb|node1| to \verb|node8|
  \item 1 x Raspberry Pi 4 Model B build node, \verb|node9|
  \item 9 x Official Raspberry Pi 4 power supplies
  \item 9 x Class 10 A1 MicroSD cards
  \item 9 x Heatsinks with integrated fans
  \item 1 x Netgear FVS318G 8 Port Gigabit Router/Firewall
  \item 1 x Netgear GS316 16 Port Gigabit Switch (with Jumbo Frame Support)
  \item Cat 7 cabling
\end{itemize}


%
% SUB SECTION
%
\subsection{Raspberry Pi 4 Model B}
The 9 x Raspberry Pi 4 Model B's used in the cluster are the 4GB RAM version. Recently, an 8GB RAM version became available. This which would be the preferred version for a future cluster.

The cluster compute nodes are \verb|node1| to \verb|node8|. These nodes are used to run benchmarks. Some benchmarks take a substantial amount of time to run, so it is convenient to have a dedicated build node for compiling software, etc, while benchmarks are running. This build node is \verb|node9|.

It is also convenient to have one of the compute nodes designated the ``master'' compute node. This is \verb|node1|. Software which needs to be compiled locally to the compute nodes, and not on the build node, is compiled on the ``master'' node. This node is also used to mirror the GitHub \verb|picluster| repository and to run the tools from the Pi Cluster Tools suite. 


%
% SUB SECTION
%
\subsection{Power Supplies}
The Raspberry Pi 4 is sensitive to voltage drops, especially whilst booting. So it was decided to use 9 \emph{Official Raspberry Pi 4} power supplies, rather than a USB hub with multiple power outlets, which may not have been able to maintain output voltage whilst booting 9 nodes. The 9 power supplies do occupy some space, so a future development would be to investigate a suitably rated USB power hub.


%
% SUB SECTION
%
\subsection{MicroSD Cards}
MicroSD cards are available in a number of speed classes and \emph{use} categories. The recommended minimum specification for the Raspberry Pi 4 is Class 10 A1. The ``10'' refers to a 10 MB/s write speed. And the ``A1'' refers to the ``Application'' category, which supports at least 1500 read operations and 500 write operations per second.


%
% SUB SECTION
%
\subsection{Heatsinks}
Cooling is a major consideration when building any cluster. The Raspberry Pi 4 Model B throttles back the clock speed at approximately 85\degree C, which would not only have had a negative impact on benchmark results, but also on repeatability. So, it was very important to select suitable cooling. After some investigation, it was decided to use heatsinks with integrated fans. These proved to be very successful, with no greater than 65\degree C observed at any time, even with 100\% CPU utilisation for many hours. 


%
% SUB SECTION
%
\subsection{Network Considerations}

The Raspberry Pi 4 Model B has a single Gigabit Ethernet interface. The theoretical maximum bandwidth of this interface is \emph{1 Gigabits per second}. As observed during benchmarking, the Raspberry Pi 4 is capable of utilising almost all of this bandwidth. It is therefore important that all networking equipment and cabling supports Gigabit Ethernet, otherwise the network performance of the cluster would be unnecessarily degraded.


%
% SUB SECTION
%
\subsection{Router/Firewall}
The router/firewall acts as the Aerin Cluster interface to the outside world. One side of the firewall is the cluster LAN (Local Area Network), on which the compute nodes and \verb|node9| are connected. The other side of the firewall is the WAN (Wide Area Network). The firewall only permits specifically configured network packets from the WAN through the firewall to the LAN. The Aerin Cluster is configured to only permit \verb|ssh| packets through the firewall, which are then routed to \verb|node1|.

The router exposes a single IP address to the WAN. Access to the cluster is through this single IP address. In my home environment the WAN is connected to my ADSL router via an Ethernet cable. This permits the compute nodes to connect to the internet and download updates. When relocated to UCL the WAN would be connected to the internal UCL network.

The router also acts as DHCP (Dynamic Host Configuration Protocol) server for the compute node LAN. Compute node hostnames, such as \verb|node1| etc, are configured by a boot script which determines the node hostname from the last octet of the node IP address, served by the DHCP server based on the MAC address. This ensures that each compute node is always assigned the same LAN IP address and hostname across reboots.

The router/firewall is easily configured through a web-based setup. Details on how to do this are included in the \verb|picluster| repository wiki.


%
% SUB SECTION
%
\subsection{Network Switch}

The network switch acts as an extension to the number of Ethernet ports on the compute node LAN. And because it supports Jumbo Frames it can accommodate an MTU increase to 9000 bytes localised to the compute nodes.


%
% SUB SECTION
%
\subsection{Cabling}
Cat 5 network cabling only support 100 Mbit/s. Cat 5e and Cat 6 supports 1 Gbit/s, but not necessarily with electrical shielding. Cat 6a and Cat 7 support 10 Gbit/s with electrical shielding. Therefore, to ensure maximum use of the network capabilities of the Raspberry Pi 4, a minimum of Cat 5e cabling must be used.

The Aerin Cluster uses Cat 7 cabling for optimum network performance.


%
% SECTION
%
\section{Software}

The Aerin Cluster consists of the following software components.


%
% SUB SECTION
%
\subsection{Operating System}
The operating system used for the Aerin Cluster is Ubuntu 20.04 LTS 64-bit Pre-Installed Server for the Raspberry Pi 4. Detailed instructions for installing the operating system are included in the \verb|picluster| repository wiki.


%
% SUB SECTION
%
\subsection{\texttt{cloud-init}}

The \verb|cloud-init| system was originally developed by Ubuntu to simplify the instantiation of operating system images in cloud computing environments, such as Amazon's AWS and Microsoft's Azure. It is now an industry standard. It can also be used for automating the installation of the same operating system on a cluster of computers using a single installation image.

The idea is that a \verb|user-data| file is added to the \verb|boot| directory of an installation image. When a node boots using the image, this file is read and the configuration/actions specified in this file are automatically applied/run as the operating system is installed.

For the Aerin Cluster the following configuration/actions were applied to each node:

\begin{itemize}
\item Add the user \verb|john| to the system and set the initial password 
\item Add \verb|john's| public key
\item Update the \verb|apt| data cache
\item Upgrade the system
\item Install specified software packages
\item Create a \verb|/etc/hosts| file
\item Set the hostname based on the IP address
\end{itemize}

All of the above is done from a single installation image and \verb|user-data| file.

The main software packages installed by \verb|cloud-init| are:

\begin{itemize}
\item build-essential
\item openmpi-bin
\item libopenblas0-serial
\item libopenblas0-openmp
\item libblis3-serial
\item libblis3-openmp
\end{itemize}

The \verb|build-essential| package installs essential software build tools, such as C/C++ compilers and \verb|make|. The \verb|openmpi-bin| package installs the OpenMPI binary and development files. And the OpenBLAS and BLIS libraries install both the serial and OpenMP versions of the respective libraries. 


%
% SUB SECTION
%
\subsection{Benchmark Software}

The HPL, HPCC and HPCG benchmark software is compiled locally from source. The instructions for how to do this are included in the \verb|picluster| repository wiki.


%
% SECTION
%
\section{BLAS Libraries}

%
% SECTION
%
\subsection{GotoBLAS}

GotoBLAS is a high performance BLAS library developed by Kazushige Goto at the Texas Advanced Computing Center (TACC), a department of the University of Texas at Austin.

GotoBLAS achieves high performance through the use of hand-crafted assembly language \emph{kernels}. Higher level BLAS routines are decomposed in \emph{kernels}, which stream data from the L1/L2/L3 CPU caches. These kernels typically reflect the size of the CPU registers, and L1/L2/L3 caches. For example, a CPU architecture may have a 4 x 4 \emph{dgemm kernel} and a 4 x 8 \emph{dgemm kernel} which conduct a double precision matrix-matrix multiplication on 4 x 4 and 4 x 8 matrices, respectively, and which have been sized for a specific architecture.

The source code for GotoBLAS and GotoBLAS2 is still available as Open Source software, but the library is no longer in active development.


%
% SECTION
%
\subsection{OpenBLAS}

OpenBLAS is an Open Source fork of the original GotoBLAS2 library, and is in active development by volunteers led by Zhang Xianyi at the Lab of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences (ISCAS).

OpenBLAS is used by many of the Top500 supercomputers, including the Fugaku supercomputer which tops the June 2020 TOP500 List.

For the Arm64 architecture, OpenBLAS implements the following \verb|dgemm| \emph{kernels}, where \verb|.S| indicates an assembly language file:

\begin{itemize}
  \item dgemm\_kernel\_4x4.S
  \item dgemm\_kernel\_4x8.S
  \item dgemm\_kernel\_8x4.S 
\end{itemize}


%
% SECTION
%
\subsection{BLIS}

The ``BLAS-like Library Instantiation Software'' (BLIS) is a BLAS library implementation for many CPU architectures, and also a framework for implementing new BLAS libraries for new architectures. Using the BLIS framework, by solely implementing an optimised \verb|dgemm| \emph{kernel} in assembly language or compiler intrinsics, BLAS library functionality can be realised which achieves 60\% - 90\% of theoretical maximum performance.

BLIS is developed by the Science of High-Performance Computing (SHPC) group of the Oden Institute for Computational Engineering and Sciences, at The University of Texas at Austin.

For the Arm64 architecture, BLIS implements the following \verb|dgemm| assembly language \emph{kernel}:

\begin{itemize}
  \item gemm\_armv8a\_asm\_6x8
\end{itemize}


%
% SUB SECTION
%
\subsection{Aerin Cluster BLAS Libraries}

To enable comparison between BLAS library implementations, OpenBLAS and BLIS, in both serial and OpenMP versions, are installed on the Aerin Cluster. For benchmark consistency, and repeatability, it is essential that the same BLAS library in use on each node at the same time. Two tools from the Pi Cluster Tools suite, \verb|libblas-query| and \verb|libblas-set|, simplify BLAS library management. 

%
% SECTION
%
\section{Cluster Topologies}

\begin{figure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.71\textwidth]{core-pure-openmpi.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.71\textwidth]{core-hybrid-openmpi-openmp.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{Single Node Toplologies.}}
\label{fig:image2}
\end{figure}


%
% SUB SECTION
%
\subsection{Pure OpenMPI}

In a pure OpenMPI topology, work is distributed across the processor cores of each node by OpenMPI. Each core runs a single MPI work process. This is depicted in Figure 4.2(a).

A work process can be \emph{bound} to a specific core. This is an optimisation which reduces \emph{cache refreshes} when a work process is interrupted and is then subsequently re-scheduled.

Each core of the Aerin Cluster supports a single thread of execution. Therefore, in this topology, work processes are linked against the serial, single-threaded, versions of the BLAS libraries.


%
% SUB SECTION
%
\subsection{Hybrid OpenMPI/OpenMP}
In a hybrid OpenMPI/OpenMP topology, OpenMPI is used to distribute work between cluster nodes. Each node runs a single MPI work process. OpenMP is then used to distribute the work of this single process between node cores. This is depicted in Figure 4.2(b).

Each node of the Aerin Cluster supports a multiple threads of execution, one on each core. Therefore, in this topology, work processes are linked against the OpenMP, multi-threaded, versions of the BLAS libraries.


%
% SECTION
%
\section{Pi Cluster Tools}

Command line administration of a cluster of computers is repetitive and prone to error. To work around this problem, and to make administration of the Aerin Cluster easier, a selection of \verb|bash| scripts were written and called \emph{Pi Cluster Tools}. Each tool loops over a list of nodes and uses \verb|ssh| to invoke a remote action on each node in turn.

The Pi Cluster Tools scripts should be invoked from \verb|node1|.

Pi Cluster Tools consists of the following tools:

\begin{itemize}
\item upgrade
\item reboot
\item shutdown
\item do
\item libblas-query
\item libblas-set
\item linpack-profiler
\item interrupt-coalescing
\item receive-packet-steering
\item receive-flow-steering
\end{itemize}

Script listings are sample usage are included in the \verb|picluster| repository wiki.

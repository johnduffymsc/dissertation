%
% SECTION
%
\section{Theoretical Maximum Performance}

The Raspberry Pi 4 Model B is based on the Broadcom BCM2711 System on a Chip (SoC). The BCM2711 includes four Arm Cortex-A72 cores clocked at 1.5 GHz.

Each core of the Arm Cortex-A72 implements the 64-bit Armv8-A ISA (Instruction Set Architecture). This instruction set includes Advanced SIMD instructions which operate on a single 128-bit SIMD pipeline. This pipeline can conduct two 64-bit double precision \emph{floating point operations} per clock cycle.  

A \emph{fused multiply-add} (FMA) instruction implements a \emph{multiplication} followed by an \emph{add} in a single instruction. The main purpose of FMA instructions is to improve result accuracy by conducting a single rounding operation on completion of both the \emph{multiplication} and \emph{add} operations. A single FMA instruction conducts two \emph{floating point operations} per clock cycle. 

The theoretical maximum performance of a single Aerin Cluster node, $R_{peak}$, is therefore:

\begin{align}
R_{peak} &= 4 \textrm{ cores} \times 1.5 \textrm{ GHz} \times 2 \textrm{ doubles} \times 2 \textrm{ FMA}\\
&= 24 \textrm{ Gflops}
\end{align}

This $R_{peak}$ of 24 Gflops is only achievable, continuously, if every instruction in a program is an FMA instruction, and the program data is aligned in memory appropriately for efficient access. This obviously cannot be the case, since a program will consist of at least some instructions to  load data from memory and store results back in memory, and these are not FMA instructions. Therefore, $R_{peak}$ is very much a theoretical maximum performance. 

The theoretical maximum performance of the Aerin Cluster as a whole is therefore:

\begin{align}
R_{peak} &= 8 \textrm{ nodes} \times 24 \textrm{ Gflops}\\
&= 192 \textrm{ Gflops}
\end{align}

For maximum performance, the HPL benchmark requires a problem size which utilises 100\% of memory. But, because the operating system requires memory, this is never fully achievable. 

\begin{figure}[h]
	\centering	
	\includegraphics[width=1.0\textwidth]{screenshot-memory.png}
	\caption{\textbf{Available Memory}. Output from \texttt{dmesg | grep Memory} indicates the memory usage by the Linux kernel, and the memory available to applications and benchmarks}
\end{figure}

As can be seen in Figure 5.1, 3.6 GB of memory (3,783,868 KB) is available to applications and benchmarks per node. This equates to 90\% of the total 4 GB (4,194,304 KB) per node. Any transient use of more than 90\% of memory will result in memory pages being swapped to permanent storage, which will negatively impact benchmark performance.

Therefore, for the HPL baseline benchmarks, 80\% of available memory was chosen for the problem size. This is also the amount suggested as an initial \emph{good guess} in the HPL Frequently Asked Questions \cite{hpl-faq}.

The above necessarily results in the baseline performance only being able to achieve 80\% of $R_{peak}$, at best. This is 4.8 Gflops for a single core, 19.2 Gflops for a single node, and 153.6 Gflops for the 8 node cluster. These values are indicated on the HPL baseline performance plots.


%
% SECTION
%
\section{HPL Baseline}

Detailed instructions on how to install the HPL benchmark software, and how to run the HPL benchmark are included in the project repository wiki.

To establish \emph{baseline} performance, the HPL benchmark was run using the default Ubuntu 20.04 LTS 64-bit packages, and without any system or network tuning.

Baseline performance was investigated for the single core, single node, two node and whole cluster configurations.


%
% SUB SECTION
%
\subsection{HPL 1 Core Baseline}

The purpose of this investigation is to determine the performance of a single core running a single \verb|xhpl| process, with the single core having exclusive access to the shared L2 cache. 

As discussed in the previous section, the HPL problem size is restricted to 80\% of available memory. In the case, 80\% of a single node's 4 GB. Using values of block size NB from 32 to 256, as suggested by HPL Frequently Asked Questions \cite{hpl-faq}, and using equation ?? to ensure the problem size N is an integer multiple of NB, results in Table 5.1 of NB and N combinations.

\begin{table}[H]
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
		\hline
		\textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} \\ 
		\hline
		32 & 18528 &  80 & 18480 & 128 & 18432 & 176 & 18480 & 224 & 18368 \\ 
		40 & 18520 &  88 & 18480 & 136 & 18496 & 184 & 18400 & 232 & 18328 \\ 
 		48 & 18528 &  96 & 18528 & 144 & 18432 & 192 & 18432 & 240 & 18480 \\
		56 & 18536 & 104 & 18512 & 152 & 18392 & 200 & 18400 & 248 & 18352 \\ 
 		64 & 18496 & 112 & 18480 & 160 & 18400 & 208 & 18512 & 256 & 18432 \\
		72 & 18504 & 120 & 18480 & 168 & 18480 & 216 & 18360 &   - &     - \\ 
 		\hline
	\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{1 Core NB and N Combinations.} Block size NB and problem size N combinations for NB between 32 and 256 using 80\% of 4 GB of memory.}
\end{table}

The benchmark results are plotted in Figure 5.2.

\begin{figure}[H]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmpi_gflops_vs_nb_1_core_80_percent_memory.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmp_gflops_vs_nb_1_core_80_percent_memory.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{HPL 1 Core $\mathbf{R_{max}}$ versus NB.}}
\label{fig:image2}
\end{figure}

Note: There is no benefit in using a hybrid OpenMPI/OpenMP topology for a single core running a single \verb|xhpl| process, as only a single thread is used. However, to ensure similar results were achieved, both a pure OpenMPI and hybrid OpenMPI/OpenMP were benchmarked.

%
% SUB SUB SECTION
%
\subsubsection{Observations}

As expected, there is no significant performance difference between the two topologies for both OpenBLAS and BLIS.

OpenBLAS and BLIS both attain 80\% $R_{peak}$. Without competition from additional cores for access to the L2 cache, both libraries are able to efficiently stream data from main memory, through the L1 and L2 caches, to the core registers for computation. 


%
% SUB SECTION
%
\subsection{HPL 1 Node Baseline}

The purpose of this investigation is to determine the performance of the 4 cores of a single node. In this case each core shares the L2 cache with the other cores, so less L2 data will be available per core. This should result in more L2 \emph{cache misses} requiring a \emph{cache load} from main memory. It is therefore anticipated that this will result in a performance reduction, per core, compared to the single core case.

As per the single core benchmark, the HPL problem size is restricted to 80\% of available memory. Again, this is 80\% of a single node's 4 GB. This results in the same NB and N combinations as the single core benchmark of Table 5.1.

The benchmark results are plotted in Figure 5.3. 

%
% SUB SUB SECTION
%
\subsubsection{Observations}

As anticipated, there is indeed a reduction in performance per core, 80\% $R_{peak}$ in no longer attained.

Pure OpenMPI topology attains a $R_{max}$ of ?? with an NB of ??.

The hybrid OpenMPI/OpenMP topology attains a $R_{max}$ of ?? with an NB of ??.

\begin{figure}[H]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmpi_gflops_vs_nb_1_node_80_percent_memory.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmp_gflops_vs_nb_1_node_80_percent_memory.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{HPL 1 Node $\mathbf{R_{max}}$ versus NB.}}
\label{fig:image2}
\end{figure}


%
% SUB SECTION
%
\subsection{HPL 2 Node Baseline}

The purpose of this baseline is to determine the performance of 2 nodes. Now, each core not only has to share access to the L2 cache, but the cache will be loaded with data less frequently due to network delays and competition between the nodes for access to network. It is therefore anticipated that this will result in a performance reduction, per node, compared to the single node case.

For this baseline the HPL problem size is restricted to 80\% of 2 nodes combined memory, 80\% of 8 GB. This results in the NB and N combinations in Table 5.2.

\begin{table}[H]
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
		\hline
		\textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} & \textbf{NB} & \textbf{N} \\ 
		\hline
		32 & 26208 &  80 & 26160 & 128 & 26112 & 176 & 26048 & 224 & 26208 \\ 
		40 & 26200 &  88 & 26136 & 136 & 26112 & 184 & 26128 & 232 & 25984 \\ 
 		48 & 26208 &  96 & 26208 & 144 & 26208 & 192 & 26112 & 240 & 26160 \\
		56 & 26208 & 104 & 26208 & 152 & 26144 & 200 & 26200 & 248 & 26040 \\ 
 		64 & 26176 & 112 & 26208 & 160 & 26080 & 208 & 26208 & 256 & 26112 \\
		72 & 26208 & 120 & 26160 & 168 & 26208 & 216 & 26136 &   - &     - \\ 
 		\hline
	\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{2 Node NB and N Combinations.}  Block size NB and problem size N combinations for NB between 32 and 256 using 80\% of 8 GB of memory}
\end{table}

The results are plotted in Figure 5.4

\begin{figure}[H]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmpi_gflops_vs_nb_2_node_80_percent_memory.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmp_gflops_vs_nb_2_node_80_percent_memory.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{HPL 2 Node $\mathbf{R_{max}}$ versus NB.}}
\label{fig:image2}
\end{figure}

%
% SUB SUB SECTION
%
\subsubsection{Observations}

...


%
% SUB SECTION
%
\subsection{HPL Whole Cluster Baseline}

This whole cluster baseline uses the optimum values of NB from the 2 node baseline.

\begin{table}[H]
\begin{center}
\begin{tabular}{ |l|c|c|c|c|c|c| } 
\hline
\textbf{BLAS Library} & \textbf{Nodes} & \textbf{N} & \textbf{NB} & \textbf{P} & \textbf{Q} & \textbf{$\mathbf{R_{max}}$} \textbf{(Gflops)} \\ 
\hline
OpenBLAS Serial & 3 & 32040 & 120 & 1 & 12 & 3.3720e+01 \\ 
                & 3 & 32040 & 120 & 2 &  6 & 3.1946e+01 \\
                & 3 & 32040 & 120 & 3 &  4 & 3.3844e+01 \\
                \cline{2-7} 
                & 4 & 36960 & 120 & 1 & 16 & 4.7742e+01 \\ 
                & 4 & 36960 & 120 & 2 &  8 & 4.9390e+01 \\ 
                \cline{2-7} 
                & 5 & 41400 & 120 & 1 & 20 & 5.6513e+01 \\ 
                & 5 & 41400 & 120 & 2 & 10 & 5.6038e+01 \\ 
                & 5 & 41400 & 120 & 4 &  5 & 5.5649e+01 \\ 
                \cline{2-7} 
                & 6 & 45360 & 120 & 1 & 24 & 6.8392e+01 \\ 
                & 6 & 45360 & 120 & 2 & 12 & 7.3856e+01 \\ 
                & 6 & 45360 & 120 & 3 &  8 & 6.9952e+01 \\ 
                \cline{2-7} 
                & 7 & 48960 & 120 & 1 & 28 & 7.8248e+01 \\ 
                & 7 & 48960 & 120 & 2 & 14 & 8.1017e+01 \\ 
                & 7 & 48960 & 120 & 4 &  7 & 8.1433e+01 \\ 
                \cline{2-7} 
                & 8 & 52320 & 120 & 1 & 32 & 8.6787e+01 \\ 
                & 8 & 52320 & 120 & 2 & 16 & 9.5517e+01 \\ 
                & 8 & 52320 & 120 & 4 &  8 & 9.5525e+01 \\ 
\hline
OpenBLAS OpenMP & 3 & 32032 & 88 & 1 & 3 & 3.7842e+01 \\ 
                \cline{2-7} 
                & 4 & 37048 & 88 & 1 & 4 & 4.8657e+01 \\ 
                \cline{2-7} 
                & 5 & 41448 & 88 & 1 & 5 & 6.0428e+01 \\ 
                \cline{2-7} 
                & 6 & 45320 & 88 & 1 & 6 & 6.8713e+01 \\ 
                & 6 & 45320 & 88 & 2 & 3 & 7.3722e+01 \\ 
                \cline{2-7} 
                & 7 & 49016 & 88 & 1 & 7 & 7.8712e+01 \\ 
                \cline{2-7} 
                & 8 & 52360 & 88 & 1 & 8 & 9.4245e+01 \\ 
                & 8 & 52360 & 88 & 2 & 4 & 9.6630e+01 \\ 
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPL Whole Cluster OpenBLAS Baseline.}}
\end{table}



\begin{table}
\begin{center}
\begin{tabular}{ |l|c|c|c|c|c|c| } 
\hline
\textbf{BLAS Library} & \textbf{Nodes} & \textbf{N} & \textbf{NB} & \textbf{P} & \textbf{Q} & \textbf{$\mathbf{R_{max}}$ (Gflops)} \\ 
\hline
BLIS Serial     & 3 & 32088 & 168 & 1 & 12 & 3.9005e+01 \\ 
                & 3 & 32088 & 168 & 2 &  6 & 3.9050e+01 \\ 
                & 3 & 32088 & 168 & 3 &  4 & 3.8958e+01 \\ 
                \cline{2-7} 
                & 4 & 36960 & 168 & 1 & 16 & 4.9694e+01 \\ 
                & 4 & 36960 & 168 & 2 &  8 & 5.4268e+01\\ 
                \cline{2-7} 
                & 5 & 41328 & 168 & 1 & 20 & 5.5398e+01 \\ 
                & 5 & 41328 & 168 & 2 & 10 & 6.5226e+01 \\ 
                & 5 & 41328 & 168 & 4 &  5 & 6.2356e+01 \\ 
                \cline{2-7} 
                & 6 & 45360 & 168 & 1 & 24 & 7.0278e+01 \\ 
                & 6 & 45360 & 168 & 2 & 12 & 7.9685e+01 \\ 
                & 6 & 45360 & 168 & 3 &  8 & 7.5475e+01 \\ 
                \cline{2-7} 
                & 7 & 48888 & 168 & 1 & 28 & 8.0168e+01 \\ 
                & 7 & 48888 & 168 & 2 & 14 & 8.7571e+01 \\ 
                & 7 & 48888 & 168 & 4 &  7 & 8.6035e+01 \\ 
                \cline{2-7} 
                & 8 & 52416 & 168 & 1 & 32 & 9.1148e+01 \\ 
                & 8 & 52416 & 168 & 2 & 16 & 1.0341e+02 \\ 
                & 8 & 52416 & 168 & 4 &  8 & 1.0190e+02 \\ 
\hline
BLIS OpenMP     & 3 & 32000 & 200 & 1 & 3 & 3.5132e+01 \\ 
                \cline{2-7} 
                & 4 & 37000 & 200 & 1 & 4 & 4.6953e+01 \\ 
                \cline{2-7} 
                & 5 & 41400 & 200 & 1 & 5 & 6.2550e+01 \\ 
                \cline{2-7} 
                & 6 & 45400 & 200 & 1 & 6 & 6.7204e+01 \\ 
                & 6 & 45400 & 200 & 2 & 3 & 7.2585e+01 \\ 
                \cline{2-7} 
                & 7 & 49000 & 200 & 1 & 7 & 8.1255e+01 \\ 
                \cline{2-7} 
                & 8 & 52400 & 200 & 1 & 8 & 9.1180e+01 \\ 
                & 8 & 52400 & 200 & 2 & 4 & 1.0041e+02 \\ 
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPL Whole Cluster BLIS Baseline.}}
\end{table}



The high value of $R_{max}$ for each node count is plotting in Figure 5.5.

\begin{figure}[H]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmpi_gflops_vs_nodes_80_percent_memory.pdf}
		\caption{Pure OpenMPI}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{openmp_gflops_vs_nodes_80_percent_memory.pdf}
		\caption{Hybrid OpenMPI/OpenMP}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{HPL $\mathbf{R_{max}}$ versus Node Count.}}
\label{fig:image2}
\end{figure}


%
% SUB SECTION
%
\subsection{Observations}

Best NB...

PxQ discussion... 1x8 vs 2x4... ethernet comment...

Iperf...

htop...

top...

perf...

cache misses...

software interrupts...

Suggests... improve network efficiency?



%
% SECTION
%
\section{HPCC Baseline}

The HPCC benchmark suite was run using all 8 nodes of the Aerin Cluster to obtain a whole cluster baseline.

Recall from Chapter 2, \emph{single} indicates the benchmark is run on a single randomly selected node, \emph{star} indicates the benchmark in run independently on all nodes, and \emph{global} indicates the benchmark is run using all nodes in a coordinated manner.

The results for each benchmark are presented below.


%
% SUB SECTION
%
\subsection{HPL}

HPL is included in HPCC. The performance results when running HPL as part of HPCC were similar to those when running HPL as a standalone benchmark.


%
% SUB SECTION
%
\subsection{DGEMM}

\begin{table}[H]
\begin{center}
\begin{tabular}{ |l|l| } 
\hline
\textbf{BLAS Library} & \textbf{Results} \\ 
\hline
OpenBLAS Serial & DGEMM\_N=5339 \\
                & StarDGEMM\_Gflops=3.59743 \\
                & SingleDGEMM\_Gflops=4.91086 \\
\hline
OpenBLAS OpenMP & DGEMM\_N=10687 \\
                & StarDGEMM\_Gflops=14.4261 \\
                & SingleDGEMM\_Gflops=14.426 \\
\hline
BLIS Serial     & DGEMM\_N=5349 \\
                & StarDGEMM\_Gflops=3.02439 \\
                & SingleDGEMM\_Gflops=4.95418 \\
\hline
BLIS OpenMP     & DGEMM\_N=10695 \\
                & StarDGEMM\_Gflops=16.3355 \\
                & SingleDGEMM\_Gflops=15.2042 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC DGEMM.}}
\end{table}

The DGEMM benchmark measures the performance of double precision real \emph{matrix-matrix multiplication}. The benchmark results are presented in Table 5.5.

For the single-threaded serial versions of the OpenBLAS and BLIS libraries the cluster consists of 32 processing cores. The \verb|SingleDGEMM_Gflops| results are per core.

For the multi-threaded OpenMP versions of the libraries, the cluster consists of 8 processing nodes. The \verb|SingleDGEMM_Gflops| results are per node.

\subsubsection{Observations}

The results are consistent with the HPL benchmarks, which spends approximately 87\% of the benchmark run time in the BLAS \verb|dgemm| subroutine. 

Of note  is the \emph{jitter} between the \emph{single} and \emph{star} results, particularly the OpenBLAS Serial and BLIS Serial results. This is explained by the fact that a \emph{single} randomly selected core running the benchmark has exclusive access to the L2 cache. The \emph{single} result is consistent with 1 Core Baseline results. In the \emph{star} case, all cores on each node are running the benchmark and have to share access to the L2 cache. The \emph{star} result is consistent with the 1 Node Baseline results.


%
% SUB SECTION
%
\subsection{STREAM}

The STREAM benchmark measures sustained memory bandwidth by performing four vector operations, \emph{Copy}, \emph{Scale}, \emph{Sum} and \emph{Triad}, on vectors which are at least 4 times the size of the L2 cache. This ensures the benchmark is measuring main memory access performance.

For the purposes of measuring pure memory bandwidth the \emph{Copy} operation is the most appropriate. This measures the copying of a vector from one memory location to another, without any computation on the vector data.   

The STREAM benchmark results are presented in table 5.6.

\begin{table}[]
\begin{center}
\begin{tabular}{ |l|l| } 
\hline
\textbf{BLAS Library} & \textbf{Results} \\ 
\hline
OpenBLAS Serial & STREAM\_VectorSize=28514400 \\
                & STREAM\_Threads=1 \\
                & StarSTREAM\_Copy=0.92926 \\
                & StarSTREAM\_Scale=0.979969 \\
                & StarSTREAM\_Add=0.902324 \\
                & StarSTREAM\_Triad=0.899619 \\
                & SingleSTREAM\_Copy=5.36868 \\
                & SingleSTREAM\_Scale=5.41684 \\
                & SingleSTREAM\_Add=4.75638 \\
                & SingleSTREAM\_Triad=4.75692 \\
\hline
OpenBLAS OpenMP & STREAM\_VectorSize=114232066 \\
                & STREAM\_Threads=1 \\
                & StarSTREAM\_Copy=4.76068 \\
                & StarSTREAM\_Scale=5.44287 \\
                & StarSTREAM\_Add=4.51713 \\
                & StarSTREAM\_Triad=4.53621 \\
                & SingleSTREAM\_Copy=5.47035 \\
                & SingleSTREAM\_Scale=5.46963 \\
                & SingleSTREAM\_Add=4.87128 \\
                & SingleSTREAM\_Triad=4.89569 \\
\hline
BLIS Serial     & STREAM\_VectorSize=28619136 \\
                & STREAM\_Threads=1 \\
                & StarSTREAM\_Copy=0.943137 \\
                & StarSTREAM\_Scale=0.989024 \\
                & StarSTREAM\_Add=0.910843 \\
                & StarSTREAM\_Triad=0.909211 \\
                & SingleSTREAM\_Copy=4.72341 \\
                & SingleSTREAM\_Scale=4.21768 \\
                & SingleSTREAM\_Add=3.90016 \\
                & SingleSTREAM\_Triad=3.94385 \\
\hline
BLIS OpenMP     & STREAM\_VectorSize=114406666 \\
                & STREAM\_Threads=1 \\
                & StarSTREAM\_Copy=5.05861 \\
                & StarSTREAM\_Scale=5.39591 \\
                & StarSTREAM\_Add=4.66044 \\
                & StarSTREAM\_Triad=4.6751 \\
                & SingleSTREAM\_Copy=5.41884 \\
                & SingleSTREAM\_Scale=5.45544 \\
                & SingleSTREAM\_Add=4.80613 \\
                & SingleSTREAM\_Triad=4.81397 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC STREAM.}}
\end{table}


\subsubsection{Observations}

As noted previously, the \emph{single} OpenBLAS Serial and BLIS Serial benchmarks relate to single core. And as indicated in the results, the maximum observed single core memory bandwidth is approximately 5.4 MB/s. The \emph{star} serial results of approximately 0.94 GB/s need to be factored by the node count to measure the whole node main memory bandwidth, which is then the same order of magnitude but reduced due to L2 cache collisions. 

For bandwidth measurement, the STREAM benchmark counts both the memory read and the memory write as a memory movement. This differs from most memory bandwidth benchmarks in which the read and write from one memory location to another count as a single memory movement. Therefore, for the \emph{Copy} operation, the benchmark results needs to be factored by 0.5 to align with other benchmarks and memory specifications.

For three out of the 4 BLAS library combinations, the observed \emph{Copy} vector operation bandwidth is approximately 5.4 GB/s.

The Raspberry Pi 4 Model B is equipped with LPDDR4–3200 SDRAM (Low-Power Double Data Rate Static DRAM), with a maximum data transfer rate of 3200 MB/s (3.2 GB/s).

The observed benchmark \emph{Copy} performance of 5.4 GB/s, when factored by 0.5 is 2.7 GB/s. This is 80\% of the maximum data transfer rate. This suggests that the Raspberry Pi 4 Model B is making good use of the available memory bandwidth. 



%
% SUB SECTION
%
\subsection{PTRANS}

PTRANS is a \emph{global} benchmark which implements the transpose of a large matrix in memory using the cluster nodes operating in parallel. The main purpose of this benchmark is to test inter-node communication performance.

The value of NB was selected to be optimum value from the 2 Node Baseline, with the corresponding value of N equating to 40\% of total memory. Since this not a computation benchmark, 40\% memory usage is sufficient to test inter-node communication without excessive run time. 



\begin{table}[]
\begin{center}
\begin{tabular}{ |l|l| } 
\hline
\textbf{BLAS Library} & \textbf{Results} \\
\hline
OpenBLAS Serial & PTRANS\_GBs=0.465891 \\
                & PTRANS\_n=26160 \\ 
                & PTRANS\_nb=120 \\
                & PTRANS\_nprow=1 \\
                & PTRANS\_npcol=32 \\
\hline
OpenBLAS OpenMP & PTRANS\_GBs=0.616885 \\
                & PTRANS\_n=26180 \\
                & PTRANS\_nb=88 \\
                & PTRANS\_nprow=2 \\
                & PTRANS\_npcol=4 \\
\hline
BLIS Serial     & PTRANS\_GBs=0.483766 \\
                & PTRANS\_n=26208 \\
                & PTRANS\_nb=168 \\
                & PTRANS\_nprow=1 \\
                & PTRANS\_npcol=32 \\
\hline
BLIS OpenMP     & PTRANS\_GBs=0.637484 \\
                & PTRANS\_n=26200 \\
                & PTRANS\_nb=200 \\
                & PTRANS\_nprow=2 \\
                & PTRANS\_npcol=4 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC PTRANS.}}
\end{table}


\subsubsection{Observations}

Both the OpenBLAS and BLIS serial benchmark runs produce similar results. Similarly, both OpenMP benchmark runs produce similar results. This is to be expected, since this benchmark is not testing BLAS library performance. The difference between the serial and OpenMP performance is related to the topology of the cluster, either a pure OpenMPI or hybrid OpenMPI/OpenMP topology.

With an average pure OpenMPI result of 0.475 GB/s, and a hybrid OpenMPI/OpenMP result of 0.627 GB/s, the inter-node communication performance is 32\% faster using the hybrid OpenMPI/OpenMP topology.

This result is to be expected, since the node count is 32 in the pure OpenMPI topology, but only 8 in the hybrid OpenMPI/OpenMP topology. Even though the amount of matrix data to be transposed remains the same, the inter-node communication is reduced. The inter-node messages may be larger, but there are fewer messages with less node addressing overhead, making the cluster network more efficient. 


%
% SUB SECTION
%
\subsection{Random Access}

The Random Access benchmark tests the integer update performance of a large array in memory. Random numbers generated from a normal distribution and the Linear Congruential Generator algorithm are used. The unit of the results is GUPs (\emph{GigaUpdates per Second}).

\begin{table}[]
\begin{center}
\begin{tabular}{ |l|l| } 
\hline
\textbf{BLAS Library} & \textbf{Results} \\ 
\hline
OpenBLAS Serial & MPIRandomAccess\_LCG\_N=2147483648 \\
                & MPIRandomAccess\_LCG\_GUPs=0.000642364 \\
                \cline{2-2} 
                & MPIRandomAccess\_N=2147483648 \\
                & MPIRandomAccess\_GUPs=0.000645338 \\
                \cline{2-2} 
                & RandomAccess\_LCG\_N=67108864 \\
                & StarRandomAccess\_LCG\_GUPs=0.00373175 \\
                & SingleRandomAccess\_LCG\_GUPs=0.00815537 \\
                \cline{2-2} 
                & RandomAccess\_N=67108864 \\
                & StarRandomAccess\_GUPs=0.00373372 \\
                & SingleRandomAccess\_GUPs=0.00837312 \\ 
\hline
OpenBLAS OpenMP & MPIRandomAccess\_LCG\_N=2147483648 \\
                & MPIRandomAccess\_LCG\_GUPs=0.000473649 \\
                \cline{2-2} 
                & MPIRandomAccess\_N=2147483648 \\
                & MPIRandomAccess\_GUPs=0.000477404 \\
                \cline{2-2} 
                & RandomAccess\_LCG\_N=268435456 \\
                & StarRandomAccess\_LCG\_GUPs=0.00580416 \\
                & SingleRandomAccess\_LCG\_GUPs=0.00582959 \\
                \cline{2-2} 
                & RandomAccess\_N=268435456 \\
                & StarRandomAccess\_GUPs=0.00614337 \\
                & SingleRandomAccess\_GUPs=0.00613214 \\
\hline
BLIS Serial     & MPIRandomAccess\_LCG\_N=2147483648 \\
                & MPIRandomAccess\_LCG\_GUPs=0.000644523 \\
                \cline{2-2} 
                & MPIRandomAccess\_N=2147483648 \\
                & MPIRandomAccess\_GUPs=0.00064675 \\
                \cline{2-2} 
                & RandomAccess\_LCG\_N=67108864 \\
                & StarRandomAccess\_LCG\_GUPs=0.00374527 \\
                & SingleRandomAccess\_LCG\_GUPs=0.00835127 \\
                \cline{2-2} 
                & RandomAccess\_N=67108864 \\
                & StarRandomAccess\_GUPs=0.00374741 \\
                & SingleRandomAccess\_GUPs=0.00820883 \\
\hline
BLIS OpenMP     & MPIRandomAccess\_LCG\_N=2147483648 \\
                & MPIRandomAccess\_LCG\_GUPs=0.000475485 \\
                \cline{2-2} 
                & MPIRandomAccess\_N=2147483648 \\
                & MPIRandomAccess\_GUPs=0.000476047 \\
                \cline{2-2} 
                & RandomAccess\_LCG\_N=268435456 \\
                & StarRandomAccess\_LCG\_GUPs=0.00580705 \\
                & SingleRandomAccess\_LCG\_GUPs=0.00578222 \\
                \cline{2-2} 
                & RandomAccess\_N=268435456 \\
                & StarRandomAccess\_GUPs=0.00614596 \\
                & SingleRandomAccess\_GUPs=0.00613275 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC Random Access.}}
\end{table}


\subsubsection{Observations}

For the \emph{global} benchmarks, the array size $N = 2,147,483,648$ represents 25\% of the cluster's total 32 GB of memory.

For the \emph{single} pure OpenMPI benchmarks, the array size $N = 67,108,864$ represents 25\% of 1 GB of memory. Each of the 4 cores of each node is allocated 1 GB of the total 4 GB of memory, and the array is selected to be 25\% of this.

And, for the \emph{single} hybrid OpenMPI/OpenMP benchmarks, the array size $N = 268,435,456$ represents 25\% of a single nodes's 4 GB of memory.

The above array sizes are determined by the benchmark and not selected by the user.

As expected, because this benchmark is not a test of computational performance, the pure OpenMPI topology performance is almost identical for both BLAS libraries. Similarly, the hybrid OpenMPI/OpenMP topology performance is almost identical for both BLAS libraries.

 


%
% SUB SECTION
%
\subsection{FFT}

The FFT benchmark...

\begin{table}[h]
\begin{center}
\begin{tabular}{ |l|c| } 
\hline
\textbf{BLAS Library} & \textbf{Results} \\ 
\hline
OpenBLAS Serial & Global Vector size: 268435456\\
                & Global Gflops:      1.569 \\
                \cline{2-2} 
                & Star Vector size:   16777216 \\
                & Star Min Gflops:    0.251138 \\
                & Star Avg Gflops:    0.314311 \\ 
                & Star Min Gflops:    0.335671 \\
                \cline{2-2}
                & Single Gflops:      0.494388 \\
\hline
OpenBLAS OpenMP & Global Vector size: 268435456 \\
                & Global Gflops:      1.347 \\
                \cline{2-2} 
                & Star Vector size:   67108864 \\
                & Star Min Gflops:    0.314714 \\
                & Star Avg Gflops:    0.344635 \\ 
                & Star Min Gflops:    0.375736 \\
                \cline{2-2}
                & Single Gflops:      0.532823 \\
\hline
BLIS Serial     & Global Vector size: 268435456 \\
                & Global Gflops:      1.563 \\
                \cline{2-2} 
                & Star Vector size:   16777216 \\
                & Star Min Gflops:    0.252763 \\
                & Star Avg Gflops:    0.312753 \\ 
                & Star Min Gflops:    0.344300 \\
                \cline{2-2}
                & Single Gflops:      0.515476\\
\hline
BLIS OpenMP     & Global Vector size: 268435456 \\
                & Global Gflops:      1.313 \\
                \cline{2-2} 
                & Star Vector size:   67108864 \\
                & Star Min Gflops:    0.315112 \\
                & Star Avg Gflops:    0.329216 \\ 
                & Star Min Gflops:    0.344110 \\
                \cline{2-2}
                & Single Gflops:      0.439117 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC FFT.}}
\end{table}


\subsubsection{Observations}

The...


%
% SUB SECTION
%
\subsection{Network Bandwidth and Latency}

The Network Bandwidth and Latency benchmark measures the time to send MPI messages between cluster processes. Latency is measured using 8 byte messages, and bandwidth is measured using 2,000,000 byte messages.

The major benchmark results are presented in table 5.10.

\begin{table}[h]
\begin{center}
\begin{tabular}{ |c| } 
\hline
\textbf{OpenBLAS Serial} \\ 
\hline
Max Ping Pong Latency:                 0.189604 msecs \\
Randomly Ordered Ring Latency:         0.192830 msecs \\
Min Ping Pong Bandwidth:             107.306358 MB/s \\
Naturally Ordered Ring Bandwidth:     61.385709 MB/s \\
Randomly  Ordered Ring Bandwidth:     16.907255 MB/s \\
\hline
\textbf{OpenBLAS OpenMP} \\ 
\hline
Max Ping Pong Latency:                 0.099864 msecs \\
Randomly Ordered Ring Latency:         0.075013 msecs \\
Min Ping Pong Bandwidth:             112.574381 MB/s \\
Naturally Ordered Ring Bandwidth:     70.511474 MB/s \\
Randomly  Ordered Ring Bandwidth:     73.119334 MB/s \\
\hline
\textbf{BLIS Serial} \\ 
\hline
Max Ping Pong Latency:                 0.191411 msecs \\
Randomly Ordered Ring Latency:         0.189172 msecs \\
Min Ping Pong Bandwidth:             107.075234 MB/s \\
Naturally Ordered Ring Bandwidth:     51.395031 MB/s \\
Randomly  Ordered Ring Bandwidth:     16.492559 MB/s \\
\hline
\textbf{BLIS OpenMP} \\ 
\hline
Max Ping Pong Latency:                 0.101356 msecs \\
Randomly Ordered Ring Latency:         0.070292 msecs \\
Min Ping Pong Bandwidth:             112.788813 MB/s \\
Naturally Ordered Ring Bandwidth:     99.948389 MB/s \\
Randomly  Ordered Ring Bandwidth:     72.585888 MB/s \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPCC Network Bandwidth and Latency.}}
\end{table}

The BLAS libraries are not used, but the results for all four library combinations are included for consistency with other benchmarks results.

A clear distinction needs to be made between Gigabits and Gigabytes/Megabytes. Using the standard \emph{byte} of 8 bits, 1 Gigabits per second (1 Gb/s) is 125 Megabytes per second (125 MB/s). This is the theoretical maximum bandwidth of the Raspberry Pi 4's Gigabit Ethernet interface.


\subsubsection{Observations}

The benchmark results are specified in Megabytes per second (MB/s).

The average OpenMP topology bandwidth is 112.682 MB/s, which is 90\% of the maximum theoretical bandwidth. The is consistent with 92.2\% of \emph{node-to-node} maximum theoretical bandwidth observed using the Linux \verb|iperf| command in Section ??.



The average serial topology bandwidth is 107.548 MB/s, which is 86\% of the maximum theoretical bandwidth. It is expected to observe a lower bandwidth with the serial topology due to the increased MPI process count compared to the the OpenMP topology. 



%
% SECTION
%
\section{HPCG Baseline}

The June 2020 HPCG List ranks 169 computer in order of HPCG benchmark performance.

Ranking number 1 is the Fugaku supercomputer.

Ranking 169 is the Spaceborne Computer. The Spaceborne Computer, onboard the International Space Station (ISS), is a 32 core system based on the Intel Xeon E5-2620 v4 8 core CPU, clocked at 2.1GHz, with an Infiniband interconnect.

An extract from the list for these two computers is in Table ??.

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c| } 
\hline
\textbf{HPCG} & \textbf{Name} & \textbf{Cores} & \textbf{HPL $\mathbf{R_{max}}$} & \textbf{HPCG}   & \textbf{Fraction} \\
\textbf{Rank} &      &       & \textbf{Pflops}        & \textbf{Pflops} & \textbf{of Peak}  \\
\hline
1 & Fugaku & 6,635,520 & 415.530  & 13.366 & 2.6\% \\
\hline
169 & Spaceborne & 32 & 0.001  & 0.000034 & 2.9\% \\
    & Computer   &    &        &          &       \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{Extract from June 2020 HPCG List.}}
\end{table}

For computers to be officially ranked in the HPCG List, the HPCG benchmark must be run for in excess of 30 minutes, using at least 25\% of available memory.

The HPCG benchmark results for the Aerin Cluster, obtained running the benchmark for 60 minutes using 75\% of available memory, are presented in Table ??.

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c| } 
\hline
\textbf{HPCG} & \textbf{Name} & \textbf{Cores} & \textbf{HPL $\mathbf{R_{max}}$} & \textbf{HPCG}   & \textbf{Fraction} \\
\textbf{Rank} &      &       & \textbf{Gflops}        & \textbf{Gflops} & \textbf{of Peak}  \\
\hline
- & OpenBLAS & 32 & 9.5525e+01 & 3.49084 & 1.8\% \\
  & Serial   &   &  &  &    \\
\hline
- & OpenBLAS & 32 & 9.6630e+01 & 2.90942 & 1.5\% \\
  & OpenMP   &   &  &  &    \\
\hline
- & BLIS     & 32 & 1.0341e+02 & 3.44246 & 1.8\% \\
  & Serial   &   &  &  &    \\
\hline
- & BLIS     & 32 & 1.0041e+02 & 2.95205 & 1.5\% \\
  & OpenMP   &   &  &  &    \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{Aerin Cluster HPCG Benchmark results.}}
\end{table}


\subsubsection{Observations}

The Aerin Cluster...


%
% SECTION
%
\section{Optimisations}


\lstset{style=type}
\begin{lstlisting}
$ sudo perf record mpirun -allow-run-as-root -np 4 xhpl
\end{lstlisting}



Running xhpl on 8 nodes using OpenBLAS...

\lstset{style=type}
\begin{lstlisting}
$ mpirun -host node1:4 ... node8:4 -np 32 xhpl
\end{lstlisting}


SHORTLY AFTER PROGRAM START...

On node1,... where we initiated...

top...

\lstset{style=type}
\begin{lstlisting}
top - 20:33:15 up 8 days,  6:02,  1 user,  load average: 4.02, 4.03, 4.00
Tasks: 140 total,   5 running, 135 sleeping,   0 stopped,   0 zombie
%Cpu(s): 72.5 us, 21.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  5.8 si,  0.0 st
MiB Mem :   3793.3 total,    330.1 free,   3034.9 used,    428.3 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.    698.7 avail Mem 

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                   
  34884 john      20   0  932964 732156   7980 R 100.3  18.8 106:40.29 xhpl                                                      
  34881 john      20   0  933692 732272   7916 R 100.0  18.9 107:29.75 xhpl                                                      
  34883 john      20   0  932932 731720   8136 R  99.3  18.8 107:33.25 xhpl                                                      
  34882 john      20   0  932932 731784   8208 R  97.7  18.8 107:33.64 xhpl                                                      
\end{lstlisting}

SOFTIRQS...


NODE 2 - 2 NODES ONLY TO SEE EFFECT...

IPERF!!!

On node8, running the top command...

\lstset{style=type}
\begin{lstlisting}
$ top
\end{lstlisting}

We can see...

\lstset{style=type}
\begin{lstlisting}
top - 18:58:44 up 8 days,  4:29,  1 user,  load average: 4.00, 3.75, 2.35
Tasks: 133 total,   5 running, 128 sleeping,   0 stopped,   0 zombie
%Cpu(s): 50.7 us, 47.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  1.4 si,  0.0 st
MiB Mem :   3793.3 total,    392.7 free,   2832.6 used,    568.0 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used.    901.1 avail Mem 

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                   
  23928 john      20   0  883880 682456   8200 R 100.0  17.6  13:14.17 xhpl                                                      
  23927 john      20   0  883988 682432   7932 R  99.7  17.6  13:12.58 xhpl                                                      
  23930 john      20   0  883912 682664   7832 R  99.7  17.6  13:17.01 xhpl                                                      
  23929 john      20   0  883880 682640   8376 R  99.3  17.6  13:16.25 xhpl  
\end{lstlisting}

Indicates that only 50.7\% of CPU time is being utilised by user programs (us), Linpack/OpenMPI...

I hypothesise that the 1.4\% of software interrupts (si) is responsible 47.8\% of CPU time in the kernel (sy) servicing these interupts...

Lets have a look at the software interrupts on the system...

\lstset{style=type}
\begin{lstlisting}
$ watch -n 1 cat /proc/softirqs
\end{lstlisting}


\lstset{style=type}
\begin{lstlisting}
Every 1.0s: cat /proc/softirqs

                    CPU0       CPU1       CPU2       CPU3
          HI:          0          1          0          1
       TIMER:  122234556   86872295   85904119   85646345
      NET_TX:  222717797     228381     147690     144396
      NET_RX: 1505715680       1132       1294       1048
       BLOCK:      63160      11906      13148      11223
    IRQ_POLL:          0          0          0          0
     TASKLET:   58902273         33          2          6
       SCHED:    3239933    3988327    2243001    2084571
     HRTIMER:       8116         55         53         50
         RCU:    6277982    4069531    4080009    3994395
\end{lstlisting}

As can be seen...

1. the majority of software interrupts are being generated by network receive (NET\_RX) activity, followed by network transmit activity (NET\_TX)...

2. these interrupts are being almost exclusively handled by CPU0...

What is there to be done?...

1. Reduce the numbers of interrupts...

1.1 Each packet produces an interrupt - interrupt coalesing...

1.2 Reduce the number of packets - increase MTU...

2.1 Share the interrupt servicing activity evenly across the CPUs...



%
% SUB SECTION
%
\subsection{Interrupt Coalescing}

As discussed in Chapter 2, each packet received by a network interface generates a hardware interrupt. This results in a context switch to the kernel to process the packet. \emph{Interrupt coalescing} delays the raising of a hardware interrupt until a specified number of packets have been received, or a specified period of time has elapsed, thereby reducing the number of context switches. This potentially improves throughput and benchmark performance.

The Pi Cluster Tools \verb|interrupt-coalescing| tool enables \emph{Adaptive RX} interrupt coalescing. In \emph{Adaptive RX} interrupt coalescing the kernel actively manages the number of packets received, and the time elapsed, before raising a hardware interrupt on the network interface receive queue.

The results of running the \verb|linpack-profiler| tool with \emph{Adaptive RX} interrupt coalescing enabled are plotted in Figure ??.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{profile_interrupt_coalescing.pdf}
	\caption{Pi Cluster Tools \texttt{linpack-profiler} $R_{max}$ with \emph{Adaptive RX} interrupt coalescing enabled.}
	\label{fig:subim1}
\end{figure}

Using the value of the block size NB which achieved optimum performance running the \texttt{linpack-profiler} tool, the HPL benchmark results using 80\% of available memory are presented in Table ??. 

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c| } 
\hline
\multicolumn{5}{|c}{} & \multicolumn{2}{|c|}{$\mathbf{R_{max}}$ \textbf{(Gflops)}} \\
\hline
\textbf{BLAS} & \textbf{N} & \textbf{NB} & \textbf{P} & \textbf{Q} & \textbf{Baseline}  & \textbf{Adaptive RX}  \\
              &            &             &            &            &                    & \textbf{Coalescing} \\
\hline
OpenBLAS Serial & 52416 & 104 & 2 & 16 &  & 9.6078e+01 \\
\hline
OpenBLAS OpenMP & 52416 & 72 & 2 & 4 &  & 9.3222e+01 \\
\hline
BLIS Serial     & 52416 & 168 & 4 & 8 &  & 1.0205e+02 \\
\hline
BLIS OpenMP     & 52416 & 144 & 2 & 4 &  & 1.0582e+02 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPL $\mathbf{R_{max}}$ with \emph{Adaptive RX} interrupt coalescing enabled.}}
\end{table}

\subsubsection{Observations}

Observations and discussion...


%
% SUB SECTION
%
\subsection{Receive Packet Steering and Receive Flow Steering}

The results of running the \verb|linpack-profiler| tool with \emph{Receive Packet Steering} and \emph{Receive Flow Steering} enabled are plotted in Figure ??.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{profile_rps_rfs.pdf}
	\caption{Pi Cluster Tools \texttt{linpack-profiler} $R_{max}$ with Receive Packet Steering and Receive Flow Sterring enabled.}
	\label{fig:subim1}
\end{figure}

Using the value of the block size NB which achieved optimum performance running the \texttt{linpack-profiler} tool, the HPL benchmark results using 80\% of available memory are presented in Table ??. 

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c| } 
\hline
\multicolumn{5}{|c}{} & \multicolumn{2}{|c|}{$\mathbf{R_{max}}$ \textbf{(Gflops)}} \\
\hline
\textbf{BLAS} & \textbf{N} & \textbf{NB} & \textbf{P} & \textbf{Q} & \textbf{Baseline}  & \textbf{Adaptive RX}  \\
              &            &             &            &            &                    & \textbf{Coalescing} \\
\hline
OpenBLAS Serial &  &  &  &  &  &  \\
\hline
OpenBLAS OpenMP &  &  &  &  &  &  \\
\hline
BLIS Serial     &  &  &  &  &  &  \\
\hline
BLIS OpenMP     &  &  &  &  &  &  \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}\textbf{HPL $\mathbf{R_{max}}$ with \emph{Receive Packet Steering} and \emph{Receive Flow Steering} enabled.}}
\end{table}


\subsubsection{Observations}

Observations and discussion...



%
% SUB SECTION
%
\subsection{Kernel Preemption Model}

The Linux kernel has 3 Preemption Models, as discussed in Chapter 2:

\begin{itemize}
\item Preemptive
\item Voluntary Preemption
\item No Forced Preemption
\end{itemize}

For scientific computing workloads the \emph{No Forced Preemption} model should be used, as suggested by the \emph{help} accompanying the Linux kernel configuration utility:

\say{This is the traditional Linux preemption model, geared towards throughput. It will still provide good latencies most of the time, but there are no guarantees and occasional longer delays are possible. Select this option if you are building a kernel for a server or scientific/computation system, or if you want to maximise the raw processing power of the kernel, irrespective of scheduling latencies.}

The kernel installed by Ubuntu 20.04 LTS 64-bit uses the \emph{Voluntary Preemption} model. To use the \emph{No Forced Preemption} model the kernel needs to be recompiled. Detailed instructions on how to do this are included in the \emph{Kernel Build With No Forced Preemption} \verb|picluster| repository wiki page.

The Pi Cluster Tools \verb|linpack-profiler| tool was run with a \emph{No Forced Preemption} kernel. The results are plotted in Figure ??.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{profile_nopreempt.pdf}
	\caption{Pi Cluster Tools \texttt{linpack-profiler} $R_{max}$ with a \emph{No Forced Preemption} kernel.}
	\label{fig:subim1}
\end{figure}

Using the parameters which achieved optimum performance when running the \texttt{linpack-profiler} tool, the HPL benchmark results using 80\% of available memory are presented in Table ??. 

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c| } 
\hline
\multicolumn{5}{|c}{} & \multicolumn{2}{|c|}{$\mathbf{R_{max}}$ \textbf{(Gflops)}} \\
\hline
\textbf{BLAS} & \textbf{N} & \textbf{NB} & \textbf{P} & \textbf{Q} & \textbf{Baseline}  & \textbf{No Forced}  \\
              &            &             &            &            &                    & \textbf{Preemption} \\
\hline
OpenBLAS Serial & 52416 & 112 & 2 & 16 &  & 9.5723e+01 \\
\hline
OpenBLAS OpenMP & 52416 &  72 & 2 &  4 &  & 9.3790e+01 \\
\hline
BLIS Serial     & 52320 & 120 & 2 & 16 &  & 1.0021e+02 \\
\hline
BLIS OpenMP     & 52320 & 160 & 2 &  4 &  &  \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:table-name}Comparison of the Baseline HPL $R_{max}$ with a \emph{No Forced Preemption} kernel.}
\end{table}


\subsubsection{Observations}

Observations and discussion...


%
% SUB SECTION
%
\subsection{Jumbo Frames}

On node2 start the Iperf server...

\lstset{style=type}
\begin{lstlisting}
$ iperf -s
\end{lstlisting}

On node1 start the Iperf client...

\lstset{style=type}
\begin{lstlisting}
$ iperf -c
\end{lstlisting}

ping tests of MTU...




iperf network speed...





\subsubsection{Jumbo Frames}

Requires a network switch capable of Jumbo frames...


On \verb|node2| create the \verb|Iperf| server...

\lstset{style=type}
\begin{lstlisting}
$ iperf -s
\end{lstlisting}

On \verb|node1| create and run the \verb|Iperf| client...

\lstset{style=type}
\begin{lstlisting}
$ iperf -i 1 -c node2
\end{lstlisting}

\lstset{style=type}
\begin{lstlisting}
------------------------------------------------------------
Client connecting to node2, TCP port 5001
TCP window size:  682 KByte (default)
------------------------------------------------------------
[  3] local 192.168.0.1 port 46216 connected with 192.168.0.2 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  1.15 GBytes   991 Mbits/sec
\end{lstlisting}


\begin{figure}
	\centering	
	\includegraphics[width=1.0\textwidth]{bandwidth_vs_mtu.pdf}
	\caption{Network Node to Node Bandwidth vs MTU.}
\end{figure}



%
% SECTION
%
\section{Power Usage}

\begin{figure}[]
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{power-netgear.jpg}
		\caption{Network equipment only.}
		\label{fig:subim1}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{power-idle.jpg}
		\caption{Aerin Cluster at idle.}
		\label{fig:subim2}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{power-benchmark.jpg}
		\caption{Aerin Cluster running benchmarks.}
		\label{fig:subim2}
	\end{subfigure}
\caption{\textbf{Aerin Cluster Power Usage.} Similar power usage was observed running either pure OpenMPI or hybrid OpenMPI/OpenMP benchmarks. The idle and benchmark power usage includes the network equipment.}
\label{fig:image2}
\end{figure}






